{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prajwal/anaconda3/envs/pytorch_gpu/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.13 (you have 1.4.10). Upgrade using: pip install --upgrade albumentations\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 \n",
    "import IPython\n",
    "from glob import glob\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tqdm\n",
    "#import seaborn as sns\n",
    "import albumentations as A\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "from torcheval.metrics import R2Score\n",
    "import wandb\n",
    "import torchvision\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rr =R2Score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7d4798b7f750>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "BASE_DIR = os.path.join(os.getcwd() , 'data')\n",
    "train_df = pd.read_csv(BASE_DIR  +  '/train.csv')\n",
    "TRAIN_VAL_SPLIT_SIZE = 0.2\n",
    "TRAIN_BATCH_SIZE = 256\n",
    "VAL_BATCH_SIZE = 256\n",
    "\n",
    "LEARNING_RATE =  1e-4\n",
    "EPOCHS = 18\n",
    "TIM_NUM_CLASS = 1408\n",
    "Normalize_transform_type = \"log_transform\"\n",
    "RANDOM_NUMBER = 42\n",
    "NUM_FLODS  = 5\n",
    "NUM_CLASSES = 6\n",
    "TRAITS_NAME = ['X4_mean', 'X11_mean', 'X18_mean', 'X26_mean', 'X50_mean', 'X3112_mean' ]\n",
    "FOLD = 0 # Which fold to set as validation data\n",
    "TARGET_IMAGE_SIZE  = 128\n",
    "T_MAX =        9\n",
    "LR_MODE = \"step\" # LR scheduler mode from one of \"cos\", \"step\", \"exp\"\n",
    "torch.manual_seed(RANDOM_NUMBER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wandb.login()\n",
    "\n",
    "# wandb.init(project=\"cs680v2_normalized\",\n",
    "#     config={\n",
    "#        \"learning_rate\": LEARNING_RATE,\n",
    "#         \"epochs\": EPOCHS,\n",
    "#         \"batch_size\" : TRAIN_BATCH_SIZE,\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DATA_DIRECTORY = os.path.join(os.getcwd(),\"data\")\n",
    "torch.cuda.empty_cache()\n",
    "# torch.cuda.set_per_process_memory_fraction(0.99, device=0)\n",
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"caching_allocator\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXTRA_COLOUMN = [\n",
    "    'WORLDCLIM_BIO1_annual_mean_temperature',\n",
    "'WORLDCLIM_BIO13.BIO14_delta_precipitation_of_wettest_and_dryest_month',\n",
    "'WORLDCLIM_BIO15_precipitation_seasonality',\n",
    "'WORLDCLIM_BIO4_temperature_seasonality',\n",
    "'SOIL_ocd_100.200cm_mean_0.01_deg',\n",
    "'WORLDCLIM_BIO12_annual_precipitation',\n",
    "'WORLDCLIM_BIO7_temperature_annual_range',\n",
    "'SOIL_cec_0.5cm_mean_0.01_deg',\n",
    "'SOIL_nitrogen_0.5cm_mean_0.01_deg',\n",
    "'SOIL_ocd_60.100cm_mean_0.01_deg',\n",
    "'SOIL_ocd_30.60cm_mean_0.01_deg',\n",
    "'SOIL_clay_100.200cm_mean_0.01_deg',\n",
    "'SOIL_soc_100.200cm_mean_0.01_deg',\n",
    "'SOIL_nitrogen_100.200cm_mean_0.01_deg',\n",
    "'SOIL_cec_100.200cm_mean_0.01_deg',\n",
    "'SOIL_clay_15.30cm_mean_0.01_deg',\n",
    "'SOIL_soc_60.100cm_mean_0.01_deg',\n",
    "'MODIS_2000.2020_monthly_mean_surface_reflectance_band_02_._month_m10',\n",
    "'SOIL_nitrogen_30.60cm_mean_0.01_deg',\n",
    "'SOIL_clay_5.15cm_mean_0.01_deg',\n",
    "'MODIS_2000.2020_monthly_mean_surface_reflectance_band_02_._month_m4',\n",
    "'MODIS_2000.2020_monthly_mean_surface_reflectance_band_02_._month_m11',\n",
    "'SOIL_ocd_0.5cm_mean_0.01_deg',\n",
    "'SOIL_ocd_5.15cm_mean_0.01_deg',\n",
    "'SOIL_cfvo_100.200cm_mean_0.01_deg',\n",
    "'MODIS_2000.2020_monthly_mean_surface_reflectance_band_02_._month_m5',\n",
    "'MODIS_2000.2020_monthly_mean_surface_reflectance_band_02_._month_m6',\n",
    "'SOIL_cfvo_30.60cm_mean_0.01_deg',\n",
    "'SOIL_cfvo_0.5cm_mean_0.01_deg',\n",
    "'SOIL_soc_15.30cm_mean_0.01_deg',\n",
    "'SOIL_nitrogen_15.30cm_mean_0.01_deg',\n",
    "'SOIL_nitrogen_5.15cm_mean_0.01_deg',\n",
    "'SOIL_clay_0.5cm_mean_0.01_deg',\n",
    "'SOIL_nitrogen_60.100cm_mean_0.01_deg',\n",
    "'SOIL_soc_0.5cm_mean_0.01_deg',\n",
    "'SOIL_ocd_15.30cm_mean_0.01_deg',\n",
    "'SOIL_ocs_0.30cm_mean_0.01_deg',\n",
    "'SOIL_clay_30.60cm_mean_0.01_deg',\n",
    "'MODIS_2000.2020_monthly_mean_surface_reflectance_band_02_._month_m3',\n",
    "'SOIL_cfvo_15.30cm_mean_0.01_deg',\n",
    "'SOIL_silt_100.200cm_mean_0.01_deg',\n",
    "'MODIS_2000.2020_monthly_mean_surface_reflectance_band_02_._month_m8',\n",
    "'SOIL_soc_30.60cm_mean_0.01_deg',\n",
    "'SOIL_cfvo_5.15cm_mean_0.01_deg',\n",
    "'MODIS_2000.2020_monthly_mean_surface_reflectance_band_02_._month_m12',\n",
    "'MODIS_2000.2020_monthly_mean_surface_reflectance_band_05_._month_m10',\n",
    "'SOIL_silt_0.5cm_mean_0.01_deg',\n",
    "'MODIS_2000.2020_monthly_mean_surface_reflectance_band_02_._month_m7',\n",
    "'SOIL_sand_100.200cm_mean_0.01_deg',\n",
    "'MODIS_2000.2020_monthly_mean_surface_reflectance_band_05_._month_m3',\n",
    "'MODIS_2000.2020_monthly_mean_surface_reflectance_band_05_._month_m12'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before # Num Train: 34690 | Num Valid: 8673\n",
      "# Num Train: 31666 | Num Valid: 8673\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "SCALAR = StandardScaler()\n",
    "\n",
    "def create_val_train_dataset(BASE_DIR,train_split_size,scalar:StandardScaler):\n",
    "    df = pd.read_csv(BASE_DIR  +  '/train.csv')\n",
    "    df[\"image_path\"] = BASE_DIR + \"/train_images/\" + df[\"id\"].astype(str) + \".jpeg\"   # add images in the training dataframe\n",
    "\n",
    "    # standardize the dataset\n",
    "    scalar.fit(df[EXTRA_COLOUMN])\n",
    "    df[EXTRA_COLOUMN] = scalar.transform(df[EXTRA_COLOUMN])\n",
    "    \n",
    "    # skf = StratifiedKFold(n_splits=NUM_FLODS, shuffle=True, random_state=42)\n",
    "\n",
    "    # # Create separate bin for each traits\n",
    "    # for i, trait in enumerate(TRAITS_NAME):\n",
    "\n",
    "    #     # Determine the bin edges dynamically based on the distribution of traits\n",
    "    #     bin_edges = np.percentile(df[trait], np.linspace(0, 100, NUM_FLODS + 1))\n",
    "    #     df[f\"bin_{i}\"] = np.digitize(df[trait], bin_edges)\n",
    "\n",
    "    # # Concatenate the bins into a final bin\n",
    "    # df[\"final_bin\"] = (\n",
    "    #     df[[f\"bin_{i}\" for i in range(NUM_CLASSES)]]\n",
    "    #     .astype(str)\n",
    "    #     .agg(\"\".join, axis=1)\n",
    "    # )\n",
    "\n",
    "    # # Perform the stratified split using final bin\n",
    "    # df = df.reset_index(drop=True)\n",
    "    # for fold, (train_idx, valid_idx) in enumerate(skf.split(df, df[\"final_bin\"])):\n",
    "    #     df.loc[valid_idx, \"fold\"] = fold\n",
    "    # sample_df = df.copy()\n",
    "    # train_df = sample_df[sample_df.fold != FOLD]\n",
    "    # val_df = sample_df[sample_df.fold == FOLD]\n",
    " \n",
    "    \n",
    "    train_df , val_df = train_test_split(df,test_size=train_split_size,shuffle=True)\n",
    "    \n",
    "\n",
    "    print(f\"before # Num Train: {len(train_df)} | Num Valid: {len(val_df)}\")\n",
    "    \n",
    "    # log_mean_traits = np.log10(train_df[TRAITS_NAME]).mean()\n",
    "    # log_std_traits = np.log10(train_df[TRAITS_NAME]).std()\n",
    "    # qt_low = log_mean_traits - 3*log_std_traits\n",
    "    # qt_high = log_mean_traits + 3*log_std_traits\n",
    "    # for column in TRAITS_NAME:\n",
    "    #     lower_quantile = train_df[column].quantile(qt_low[column])\n",
    "    #     upper_quantile = train_df[column].quantile(qt_high[column])  \n",
    "    #     train_df = train_df[(train_df[column] >= lower_quantile) & (train_df[column] <= upper_quantile)]\n",
    "    train_df = train_df[(np.abs(stats.zscore(np.log10(train_df[TRAITS_NAME]))<3).all(axis=1)  )]\n",
    "    train_df.reset_index(drop=True , inplace  = True)\n",
    "    val_df.reset_index(drop = True , inplace = True)\n",
    "    print(f\"# Num Train: {len(train_df)} | Num Valid: {len(val_df)}\")\n",
    "    return  train_df , val_df\n",
    "def create_test_dataset(BASE_DIR,scalar):\n",
    "    df = pd.read_csv(BASE_DIR  +  '/test.csv')\n",
    "    df[\"image_path\"] = BASE_DIR + \"/test_images/\" + df[\"id\"].astype(str) + \".jpeg\"   # add images in the training dataframe\n",
    "    df[EXTRA_COLOUMN] = scalar.transform(df[EXTRA_COLOUMN])\n",
    "    return df\n",
    "\n",
    "train_df , val_df = create_val_train_dataset(BASE_DIR,TRAIN_VAL_SPLIT_SIZE,scalar = SCALAR)\n",
    "test_df = create_test_dataset(BASE_DIR,scalar=SCALAR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df[(np.abs(stats.zscore(np.log10(train_df[TRAITS_NAME]))<3).all(axis=1)  )].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeOutput():\n",
    "    def __init__(self,type_transform ):\n",
    "        self.transform = type_transform\n",
    "        self.mean = 0\n",
    "        self.std = 0\n",
    "        self.mean_tensor = 0\n",
    "        self.std_tensor = 0\n",
    "        \n",
    "        pass\n",
    "    def normalize(self,df):\n",
    "        if self.transform == \"log_transform\":\n",
    "            mean = np.log10(df).mean()\n",
    "            std = np.log10(df).std()\n",
    "            min =  np.log10(df).min()\n",
    "            max = np.log10(df).max()\n",
    "            \n",
    "            self.min  = min\n",
    "            self.max = max\n",
    "            \n",
    "            self.mean = mean\n",
    "            self.std = std\n",
    "            \n",
    "            self.min_tensor = torch.Tensor(self.min.values).to(DEVICE)\n",
    "            self.max_tensor = torch.Tensor(self.max.values).to(DEVICE)\n",
    "            \n",
    "            self.mean_tensor = torch.Tensor(self.mean.values).to(DEVICE)\n",
    "            self.std_tensor = torch.Tensor(self.std.values).to(DEVICE)\n",
    "            \n",
    "            tf = (np.log10(df) - self.min)/(self.max - self.min)\n",
    "        elif self.transform == \"log_transform_mean_std\": \n",
    "            tf = (np.log10(df) - self.mean)/(self.std) \n",
    "        \n",
    "        return tf \n",
    "    def denormalize(self,df):\n",
    "        if self.mean is None or self.std is None :\n",
    "            raise Exception(\"mean and/std is not defined \")\n",
    "        if self.normalize == \"log_transform\":\n",
    "            df_denormalize =10**((df*(self.max - self.min)) + self.min )\n",
    "            return df_denormalize\n",
    "        if self.normalize ==\"log_transform_mean_std\":\n",
    "            df_denormalize =10**((df*self.std) + self.mean )\n",
    "            return df_denormalize\n",
    "    def denormalize_tensor(self,batch) :\n",
    "        if self.mean_tensor is None or self.std_tensor is None :\n",
    "            raise Exception(\"mean and/std is not defined \")\n",
    "        if self.transform == \"log_transform\":\n",
    "            df_denormalize =10**((batch*(self.max_tensor - self.min_tensor)) + self.min_tensor )\n",
    "        elif self.transform == \"log_transform_mean_std\":\n",
    "            df_denormalize =10**((batch*self.std_tensor) + self.mean_tensor )\n",
    "        return df_denormalize\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df[['X4_mean', 'X11_mean', 'X18_mean', 'X26_mean', 'X50_mean', 'X3112_mean' ]].min().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to avoid overfitting : 1) try diffferent transformations 2) batch norm 3)  dropout  4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations.pytorch import ToTensorV2\n",
    "IMAGE_SIZE =128\n",
    "class data_loader(Dataset ):\n",
    "    def __init__(self,df , is_val = False,normalizedOutput=None, extra_params = False ):\n",
    "        global EXTRA_COLOUMN\n",
    "        self.df = df.copy()\n",
    "        self.is_val = is_val\n",
    "        if normalizedOutput == None:\n",
    "            pass\n",
    "        else:\n",
    "            if not is_val :\n",
    "                self.df[['X4_mean', 'X11_mean', 'X18_mean', 'X26_mean', 'X50_mean', 'X3112_mean' ]] = normalizedOutput.normalize(self.df[['X4_mean', 'X11_mean', 'X18_mean', 'X26_mean', 'X50_mean', 'X3112_mean' ]])\n",
    "\n",
    "        self.train_transform = A.Compose([  \n",
    "                    A.RandomResizedCrop( height = 100 , width = 100 , p=0.4 , interpolation = cv2.INTER_CUBIC),\n",
    "                    A.Resize(TARGET_IMAGE_SIZE, TARGET_IMAGE_SIZE,interpolation= cv2.INTER_CUBIC),\n",
    "                    A.ISONoise(color_shift=(0.03, 0.07), intensity=(0.3, 0.5), always_apply=None, p=0.3) ,\n",
    "                    A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.3 , p = 0.4),\n",
    "                    A.HorizontalFlip(p=0.6),\n",
    "                    #A.Blur(blur_limit=4, p=0.5),\n",
    "                    # A.VerticalFlip(p=0.1),\n",
    "                    A.ImageCompression(quality_lower=25, quality_upper=100, p=0.5),\n",
    "                    A.ToFloat(),\n",
    "                    A.Normalize([0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225], max_pixel_value=1),\n",
    "                    ToTensorV2(),\n",
    "                    ])\n",
    "        self.val_transform = A.Compose([\n",
    "                A.ToFloat(),\n",
    "                    A.Normalize([0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225], max_pixel_value=1),\n",
    "                    ToTensorV2(),\n",
    "        ])\n",
    "        self.extra_params = extra_params\n",
    "        self.extra_coloumns = EXTRA_COLOUMN\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self , index):\n",
    "        row = self.df.iloc[index]\n",
    "        image = plt.imread(row[\"image_path\"])\n",
    "        #image = np.copy(image)\n",
    "        traits = row[['X4_mean', 'X11_mean', 'X18_mean', 'X26_mean', 'X50_mean', 'X3112_mean' ]].values.astype(np.float32)\n",
    "        traits = torch.tensor(traits,dtype=torch.float32)\n",
    "        if not self.is_val:\n",
    "            image = self.train_transform(image = image)[\"image\"]\n",
    "            #image = torch.permute(image  )\n",
    "        else:\n",
    "            image = self.val_transform(image = image)[\"image\"]\n",
    "            \n",
    "        if self.extra_params:\n",
    "            extras = row[self.extra_coloumns].values.astype(np.float32)\n",
    "            return image , traits  , torch.tensor(extras,dtype=torch.float32)\n",
    "        return  image , traits\n",
    "normalize_function = NormalizeOutput(Normalize_transform_type)\n",
    "train_dataset = data_loader(train_df, is_val = False,normalizedOutput=normalize_function ,extra_params=True)\n",
    "val_dataset  = data_loader(val_df,   is_val = True,normalizedOutput=normalize_function , extra_params=True)    \n",
    "\n",
    "train_dataloader = DataLoader(train_dataset , batch_size = TRAIN_BATCH_SIZE , shuffle=True )\n",
    "val_dataloader = DataLoader(val_dataset , batch_size =  VAL_BATCH_SIZE , shuffle=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(train_dataset))[1].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class R2Loss(nn.Module):\n",
    "    def __init__(self, num_classes=6):\n",
    "        super(R2Loss, self).__init__()\n",
    "        # Initialize learnable weights for each class, one weight per class\n",
    "        # self.class_weights = nn.Parameter(torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 1.0], dtype=torch.float32))\n",
    "        # Increase weight for X_26_mean\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Calculate residual sum of squares per class\n",
    "        SS_res = torch.sum((y_true - y_pred) ** 2, dim=0)  # (B, C) -> (C,)\n",
    "        # Calculate total sum of squares per class\n",
    "        SS_tot = torch.sum(\n",
    "            (y_true - torch.mean(y_true, dim=0)) ** 2, dim=0\n",
    "        )  # (B, C) -> (C,)\n",
    "        # Calculate R2 loss per class, avoiding division by zero\n",
    "        r2_loss =SS_res / (SS_tot + 1e-6)  # (C,)\n",
    "        return torch.mean(r2_loss)\n",
    "\n",
    "# class R2Metric(nn.Module):\n",
    "#     def __init__(self, name=\"r2\", **kwargs):\n",
    "#         super(R2Metric, self).__init__()\n",
    "#         self.SS_res = self.add_weight(name='SS_res', shape=(6,), initializer='zeros')\n",
    "#         self.SS_tot = self.add_weight(name='SS_tot', shape=(6,) ,initializer='zeros')\n",
    "#         self.num_samples = self.add_weight(name='num_samples', initializer='zeros')\n",
    "\n",
    "#     def update(self, y_pred, y_true, sample_weight=None):\n",
    "#         SS_res = torch.sum(torch.square(y_true - y_pred), axis=0)\n",
    "#         SS_tot = torch.sum(torch.square(y_true - torch.mean(y_true, axis=0)), axis=0)\n",
    "#         self.SS_res.assign_add(SS_res)\n",
    "#         self.SS_tot.assign_add(SS_tot)\n",
    "#         self.num_samples.assign_add(torch.cast(torch.shape(y_true)[0], \"float32\"))\n",
    "\n",
    "#     def compute(self):\n",
    "#         r2 = 1 - self.SS_res / (self.SS_tot + 1e-6)\n",
    "#         return torch.mean(r2)   \n",
    "\n",
    "\n",
    "def initialize_timm_model( model_name   , num_class):\n",
    "    model_ft  = None\n",
    "    if model_name == \"resnet34\" :\n",
    "        \"\"\" Resnet34 \"\"\"\n",
    "        model = timm.create_model('resnet34' , num_classes=num_class )\n",
    "        return model\n",
    "    if model_name == \"Swin_Transformer\":\n",
    "        model = timm.create_model('swin_tiny_patch4_window7_224.ms_in22k' , pretrained=True , num_classes = num_class)\n",
    "        return model \n",
    "    if model_name ==\"convnextv2\":\n",
    "        model = timm.create_model('convnext_tiny.in12k_ft_in1k_384',num_classes=num_class)\n",
    "        return model \n",
    "    \n",
    "    if model_name == \"efficientnet_v2\":\n",
    "        model = timm.create_model(\"efficientnet_b2.ra_in1k\",pretrained = True)\n",
    "        model.classifier = nn.Dropout(p=0.2,inplace=False)\n",
    "        return model\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_channels,tim_num_class , model):\n",
    "        super().__init__()\n",
    "        # if model ==\"resnet34\":\n",
    "        self.backbone = initialize_timm_model(model_name=model ,num_class=tim_num_class)\n",
    "        self.extra_parameters_models = nn.Sequential(\n",
    "            nn.Linear(input_channels,input_channels*2),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(input_channels*2,256),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(256,128),\n",
    "            nn.SELU(),\n",
    "            nn.Dropout(p=0.4,inplace=False), #\n",
    "            nn.Linear(128,64),\n",
    "            nn.SELU(),\n",
    "            nn.Dropout(p=0.3,inplace=False)\n",
    "            \n",
    "        )\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(tim_num_class+64,736),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(736,368)     ,  \n",
    "            nn.SELU(),\n",
    "            nn.Linear(368,184)   , \n",
    "            nn.Dropout(p=0.1,inplace=False),\n",
    "            nn.SELU(),\n",
    "            nn.Linear(184,92),\n",
    "            nn.Dropout(p=0.4,inplace=False), #\n",
    "            nn.SELU(),\n",
    "            nn.Linear(92,6)\n",
    "        )\n",
    "        \n",
    "    def forward(self,image,x):\n",
    "        output_image = self.backbone(image) # bach * (hight*col)\n",
    "        z = self.extra_parameters_models(x) # batch * 16\n",
    "        inputs  = torch.cat((output_image,z), 1 )\n",
    "        output = self.output(inputs)\n",
    "        return output\n",
    "\n",
    "def get_model_optimizer_lossFunction(model_name,learning_rate,extra_params = False):\n",
    "    global DEVICE\n",
    "    if extra_params:\n",
    "        model = CustomModel(input_channels=len(EXTRA_COLOUMN),tim_num_class= TIM_NUM_CLASS, model=model_name)\n",
    "    else:\n",
    "        model = initialize_timm_model(model_name=model_name , TIM_NUM_CLASS = 6)    \n",
    "    model.to(device = DEVICE)\n",
    "    loss_function = R2Loss()\n",
    "    loss_function.to(device=DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay= 0.01)\n",
    "    # scheduler  = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.1 , patience=1 , verbose = True )\n",
    "    scheduler  = torch.optim.lr_scheduler.CosineAnnealingLR( optimizer , T_max = T_MAX , eta_min= 1e-5 )\n",
    "\n",
    "    return model,optimizer,loss_function , scheduler\n",
    "class BestModelSaveCallback:\n",
    "    def __init__(self, save_path):\n",
    "        self.save_path = save_path\n",
    "        self.best_accuracy = -1\n",
    "\n",
    "    def __call__(self, accuracy,model):\n",
    "        if accuracy > self.best_accuracy:\n",
    "            self.best_accuracy = accuracy\n",
    "            model.to(device = \"cpu\")\n",
    "            torch.save(model.state_dict(), self.save_path)\n",
    "            model.to(device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = initialize_timm_model(\"efficientnet_v2\" , 128)\n",
    "# image = torch.ones(10,3,224,224)\n",
    "# model(image).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model  = CustomModel(input_channels=163,tim_num_class=1408 , model=\"efficientnet_v2\")\n",
    "# image = torch.ones(10,3,224,224)\n",
    "# x = torch.ones(10,len(EXTRA_COLOUMN))\n",
    "# model(image,x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(inputs,model,loss_function,optimizer,extra_params = False):\n",
    "    model.train()  \n",
    "    if extra_params :\n",
    "        x,y,z = inputs\n",
    "        x = x.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "        z = z.to(DEVICE)\n",
    "        prediction = model(x,z)        \n",
    "        \n",
    "    else:\n",
    "        x,y = inputs\n",
    "        x = x.to(DEVICE)\n",
    "        y = y.to(DEVICE)    \n",
    "        prediction = model(x)\n",
    "    \n",
    "    \n",
    "    #loss_func = nn.MSELoss()\n",
    "    #loss_val = loss_func(prediction,y)\n",
    "    loss_1 = loss_function(prediction,y) \n",
    "    loss =  loss_1 #+ 0.5 * loss_val\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    return loss_1.detach().cpu().numpy()\n",
    "\n",
    "@torch.no_grad\n",
    "def do_prediction(inputs,model, is_val=False , extra_params = False):\n",
    "    global Train_std_tensor , Train_mean_tensor\n",
    "    model.eval()\n",
    "    if extra_params:\n",
    "        x,y,z = inputs\n",
    "        x = x.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "        z = z.to(DEVICE)\n",
    "        \n",
    "        prediction = model(x,z)\n",
    "    else:\n",
    "        x,y = inputs\n",
    "        x = x.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "        prediction = model(x)\n",
    "    if is_val :\n",
    "        prediction = normalize_function.denormalize_tensor(batch=prediction)\n",
    "    return prediction.detach().cpu().numpy()\n",
    "\n",
    "@torch.no_grad()\n",
    "def validation_loss_batch(inputs,model,loss_function,extra_params=False):\n",
    "    global Train_std_tensor , Train_mean_tensor\n",
    "    model.eval()\n",
    "    if extra_params:\n",
    "        x,y,z = inputs\n",
    "        x = x.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "        z = z.to(DEVICE)\n",
    "        prediction = model(x,z)\n",
    "    else:\n",
    "        x,y = inputs\n",
    "        x = x.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "        prediction = model(x)\n",
    "    prediction = normalize_function.denormalize_tensor(batch=prediction)\n",
    "    loss = loss_function(prediction, y)\n",
    "    return loss.detach().cpu().numpy()\n",
    "\n",
    "def utils_convert_to_2d_tensors(predictions,targets):\n",
    "    predictions  = np.array(predictions)\n",
    "    targets = np.array(targets)\n",
    "    predictions  = np.reshape(predictions , (-1, predictions.shape[-1]))\n",
    "    targets  = np.reshape(targets  , (-1 , targets.shape[-1]))\n",
    "    return torch.Tensor(predictions), torch.Tensor(targets)\n",
    "\n",
    "def train(trainLoader,valLoader,model,optimizer,loss_function,epochs,best_model_callback,extra_params=False):\n",
    "    #wandb.watch(model,loss_function,log = \"all\",log_freq=50)\n",
    "    \n",
    "    train_epoch_loss , train_epoch_accuracy =[] , []\n",
    "    val_epoch_loss , val_epoch_accuracy = [],[]\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"epoch: {epoch} , lr is { scheduler.get_last_lr()}\" )\n",
    "        train_loss  = [] \n",
    "        val_loss ,val_accuracy = [] , []\n",
    "        \n",
    "        # batch training loss\n",
    "        with tqdm.tqdm(total=len(trainLoader)) as trainingLoop:\n",
    "            for index,batch in enumerate(iter(trainLoader)): \n",
    "        \n",
    "                loss = train_batch(batch,model,loss_function,optimizer,extra_params=extra_params)\n",
    "                train_loss.append(loss)\n",
    "\n",
    "                trainingLoop.set_description(f\"Batch: {index}/{len(trainLoader)}\")\n",
    "                trainingLoop.set_postfix({\"training Loss \" : loss})\n",
    "                trainingLoop.update(1)\n",
    "                ##wandb.log({\"Training Loss\":loss })\n",
    "        train_loss  = np.array(train_loss).mean() \n",
    "        train_epoch_loss.append(train_loss)\n",
    "        \n",
    "        # find training accuracy \n",
    "        predictions,targets = [],[]\n",
    "        for index,batch in enumerate(iter(trainLoader)): \n",
    "        \n",
    "            prediction = do_prediction(batch,model,extra_params=extra_params)\n",
    "            predictions.extend(prediction)\n",
    "            targets.extend(batch[1].detach().cpu().numpy())\n",
    "           \n",
    "        \n",
    "        predictions = np.array(predictions)\n",
    "        targets = np.array(targets)\n",
    "        predictions , targets = utils_convert_to_2d_tensors(predictions , targets)\n",
    "        # print(\"predictions=\", predictions[0:2])\n",
    "        # print(\"targets=\",targets[0:2])\n",
    "        metric = R2Score()\n",
    "        metric.update(predictions , targets)\n",
    "        train_epoch_accuracy.append(metric.compute().detach().cpu().numpy())\n",
    "                \n",
    "        \n",
    "        # validation set loss & accuracy  \n",
    "        predictions,targets = [],[]\n",
    "        with tqdm.tqdm(total = len(valLoader)) as validationLoop:\n",
    "            for index,batch in enumerate(iter(valLoader)):\n",
    "                \n",
    "                loss = validation_loss_batch(batch,model,loss_function,extra_params=extra_params)\n",
    "                val_loss.append(loss)\n",
    "                prediction = do_prediction(batch,model,is_val=True,extra_params=extra_params)\n",
    "                predictions.extend(prediction)\n",
    "                targets.extend(batch[1].detach().cpu().numpy())\n",
    "                \n",
    "                validationLoop.set_description(f\"Batch: {index}/{len(valLoader)}\")\n",
    "                validationLoop.set_postfix({\"Validation loss \" : loss}) \n",
    "                ##wandb.log({\"Vlaidation loss\" : loss})\n",
    "                #wandb.log({\"Validation Loss \": val_loss.item()})\n",
    "                validationLoop.update(1)\n",
    "        \n",
    "        \n",
    "        val_loss  = np.array(val_loss).mean() \n",
    "        val_epoch_loss.append(val_loss)\n",
    "        \n",
    "        predictions = np.array(predictions)\n",
    "        targets = np.array(targets)\n",
    "        # print(\"predictions-val=\", predictions[0:2])\n",
    "        # print(\"targets-val=\",targets[0:2])\n",
    "        metric =R2Score()\n",
    "        predictions , targets = utils_convert_to_2d_tensors(predictions , targets)\n",
    "        metric.update(predictions , targets)\n",
    "        val_epoch_accuracy.append(metric.compute().detach().cpu().numpy().item())\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f\"epoch:{epoch}, Training (avg) loss : {train_loss} , Validation loss (avg) = {val_loss}\")\n",
    "        print(f\"epoch:{epoch}, Training (avg) Accuracy : {train_epoch_accuracy[-1]} , Validation Accuracy (avg) = {val_epoch_accuracy[-1]}\")\n",
    "        best_model_callback(metric.compute().detach().cpu().numpy().item(),model)        # save the best model according to the validation accuracy\n",
    "        \n",
    "        \n",
    "    return train_epoch_loss,val_epoch_loss,train_epoch_accuracy , val_epoch_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = CustomModel(len(EXTRA_COLOUMN),6 , \"resnet34\")\n",
    "# image = torch.ones(10,3,128,128)\n",
    "\n",
    "# # x-shape= torch.Size([10, 3, 128, 128]) z-shape= torch.Size([10, 163]\n",
    "# x = torch.ones(10,len(EXTRA_COLOUMN))\n",
    "# model(image,x).shape\n",
    "# model = initialize_timm_model(model_name=\"efficientnet_v2\",num_class=6)\n",
    "#model,optimizer,loss_function,scheduler = get_model_optimizer_lossFunction(model_name = \"efficientnet_v2\",learning_rate = LEARNING_RATE,extra_params=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:timm.models._builder:Loading pretrained weights from Hugging Face hub (timm/efficientnet_b2.ra_in1k)\n",
      "INFO:timm.models._hub:[timm/efficientnet_b2.ra_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MODEL_NAME_SAVE = 'best_model_transformation_efficientnet_selected_features.pth'\n",
    "model,optimizer,loss_function,scheduler = get_model_optimizer_lossFunction(model_name = \"efficientnet_v2\",learning_rate = LEARNING_RATE,extra_params=True)\n",
    "# best_model_callback = BestModelSaveCallback(save_path=os.path.join(BASE_DIR,MODEL_NAME_SAVE))\n",
    "# train_losses, val_losses , train_accuracies,val_accuracies = train(train_dataloader,val_dataloader,model,optimizer,loss_function,EPOCHS,best_model_callback,extra_params=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20334/1483391290.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f\"/home/prajwal/cs680/cs680_kaggle/data/{MODEL_NAME_SAVE}\"))\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME_SAVE = 'best_model_transformation_efficientnet_selected_features.pth'\n",
    "model.load_state_dict(torch.load(f\"/home/prajwal/cs680/cs680_kaggle/data/{MODEL_NAME_SAVE}\"))\n",
    "TEST_BATCH_SIZE  = 10\n",
    "torch.cuda.empty_cache()\n",
    "class data_loader_test(Dataset ):\n",
    "    def __init__(self,df ,extra_params = False ):\n",
    "        global EXTRA_COLOUMN\n",
    "        self.df = df.copy()\n",
    "        self.transform =torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                                 #torchvision.transforms.Resize((384,384)),\n",
    "                                  torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "                                  ])\n",
    "    \n",
    "        self.extra_params = extra_params\n",
    "        self.extra_coloumns = EXTRA_COLOUMN\n",
    "    def __len__(self):\n",
    "        return len(self.df)    \n",
    "    def __getitem__(self , index):\n",
    "        row = self.df.iloc[index]\n",
    "        image = plt.imread(row[\"image_path\"])\n",
    "        image = np.copy(image)\n",
    "        if self.transform :\n",
    "            image = self.transform(image)\n",
    "        if self.extra_params:\n",
    "            extras = row[self.extra_coloumns].values.astype(np.float64)\n",
    "            return image  , torch.Tensor(extras)\n",
    "        return  image \n",
    "test_dataset = data_loader_test(test_df,extra_params = True)\n",
    "test_dataloader = DataLoader(test_dataset,batch_size =TEST_BATCH_SIZE,shuffle=False)\n",
    "\n",
    "def predict_test(test_dataloader , model):\n",
    "    predictions = []\n",
    "    for i in test_dataloader:\n",
    "        #print(i[0].shape , i[1].shape)\n",
    "        model.eval()\n",
    "        prediction = model(i[0].to(DEVICE),i[1].to(DEVICE))\n",
    "        prediction  = normalize_function.denormalize_tensor(prediction)\n",
    "        predictions.extend(prediction.detach().cpu().numpy())\n",
    "    predictions = np.array(predictions)\n",
    "    output = np.reshape(predictions,(-1,predictions.shape[-1]))\n",
    "    return pd.DataFrame(output , columns=['X4', 'X11', 'X18', 'X26', 'X50', 'X3112' ])\n",
    "output = predict_test(test_dataloader , model)\n",
    "output = pd.concat([test_df[\"id\"],output],axis=1 )\n",
    "#output.to_csv()\n",
    "output.to_csv(\"best_model_transformation_efficientnet_selected_features.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# predictions= torch.tensor([[ 1.2391, 30.3367, 30.3599, 29.7099, 20.0526, 31.2610],\n",
    "#         [ 1.2679, 30.3845, 30.3134, 29.7083, 20.3929, 31.2463]])\n",
    "# targets= torch.tensor([[1.1125e+00, 1.4686e+02, 1.9699e+04, 3.4597e+03, 1.5282e+01, 3.9792e+05],\n",
    "#         [9.7378e-01, 1.5390e+02, 1.9702e+04, 3.4673e+03, 1.4737e+01, 3.9847e+05]])\n",
    "# metric = R2Score()\n",
    "# metric.update(predictions , targets)\n",
    "# metric.compute()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from sklearn.metrics import r2_score\n",
    "# predictions= np.array([[ 1.2391, 30.3367, 30.3599, 29.7099, 20.0526, 31.2610],\n",
    "#         [ 1.2679, 30.3845, 30.3134, 29.7083, 20.3929, 31.2463]])\n",
    "# targets= np.array([[1.1125e+00, 1.4686e+02, 1.9699e+04, 3.4597e+03, 1.5282e+01, 3.9792e+05],\n",
    "#         [9.7378e-01, 1.5390e+02, 1.9702e+04, 3.4673e+03, 1.4737e+01, 3.9847e+05]])\n",
    "# r2_score(y_true = targets , y_pred =predictions)\n",
    "\n",
    "\n",
    "\n",
    "# targets = torch.tensor([[0.0, 2.0], [1.0, 6.0]])\n",
    "# predictions = torch.tensor([[0.0, 1.0], [2.0, 5.0]])\n",
    "# torch.mean(1- (torch.sum((targets - predictions)**2,axis=0)/torch.sum((targets - targets.mean())**2,axis=0)))\n",
    "\n",
    "\n",
    "# metric = R2Score()\n",
    "# # input = torch.tensor([[0, 2], [1, 6]])\n",
    "# # target = torch.tensor([[0, 1], [2, 5]])\n",
    "# input = torch.tensor([[ 1.2391, 30.3367, 30.3599, 29.7099, 20.0526, 31.2610],\n",
    "#         [ 1.2679, 30.3845, 30.3134, 29.7083, 20.3929, 31.2463]])\n",
    "# target= torch.tensor([[1.1125e+00, 1.4686e+02, 1.9699e+04, 3.4597e+03, 1.5282e+01, 3.9792e+05],\n",
    "#         [9.7378e-01, 1.5390e+02, 1.9702e+04, 3.4673e+03, 1.4737e+01, 3.9847e+05]])\n",
    "# metric.update(input, target)\n",
    "# metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Plot(train_losses,train_accuracies,val_losses,val_accuracies,path):\n",
    "    plt.plot(train_losses,label = \"train loss\")\n",
    "    plt.plot(val_losses,label = \"validation loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    #plt.savefig(os.path.join(path,\"Loss.png\"))\n",
    "    #wandb.log({\"Loss\": plt})\n",
    "\n",
    "    plt.plot(val_accuracies,label = \"validation accuracy\")\n",
    "    plt.plot(train_accuracies,label = \"train accuracy\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend() \n",
    "    #plt.savefig(os.path.join(path,\"Accuracy.png\"))\n",
    "    #wandb.log({\"Accuracy\": plt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m Plot(train_losses,train_accuracies,val_losses,val_accuracies,path \u001b[38;5;241m=\u001b[39m BASE_DIR)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_losses' is not defined"
     ]
    }
   ],
   "source": [
    "Plot(train_losses,train_accuracies,val_losses,val_accuracies,path = BASE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7b816e22fe90>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(val_accuracies,label = \"validation accuracy\")\n",
    "plt.plot(train_accuracies,label = \"train accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
