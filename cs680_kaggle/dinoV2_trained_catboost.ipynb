{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.13 (you have 1.4.10). Upgrade using: pip install --upgrade albumentations\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 \n",
    "import IPython\n",
    "from glob import glob\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tqdm\n",
    "#import seaborn as sns\n",
    "import albumentations as A\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "import torcheval \n",
    "import wandb\n",
    "import torchvision\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams.update(**{'figure.dpi':150})\n",
    "import psutil\n",
    "import imageio.v3 as imageio\n",
    "\n",
    "# %%\n",
    "RANDOM_NUMBER = 42\n",
    "torch.manual_seed(RANDOM_NUMBER)\n",
    "\n",
    "# %% [markdown]\n",
    "# # select Device\n",
    "\n",
    "# %%\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DATA_DIRECTORY = os.path.join(os.getcwd(),\"data\")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# %%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler , MinMaxScaler\n",
    "import scipy as sp\n",
    "#pid :279026 \n",
    "# %%\n",
    "class Config():\n",
    "\n",
    "    BASE_DIR = os.path.join(os.getcwd() , 'data')\n",
    "    train_df = pd.read_csv(BASE_DIR  +  '/train.csv')\n",
    "    TRAIN_VAL_SPLIT_SIZE = 0.01\n",
    "    TRAIN_BATCH_SIZE =2\n",
    "    VAL_BATCH_SIZE = 2\n",
    "    TEST_BATCH_SIZE = 2\n",
    "    LR_MAX = 1e-4 \n",
    "    NUM_EPOCHS = 20\n",
    "    IMG_OUT_FEATURES = 768  #512  # swin     #768<-vit   \n",
    "    NORMALIZE_TARGET = \"log_transform_mean_std\"   #\"log_transform\" #\n",
    "    RANDOM_NUMBER = 42\n",
    "    NUM_FLODS  = 5\n",
    "    NUM_CLASSES = 6\n",
    "    TRAITS_NAME = ['X4_mean', 'X11_mean', 'X18_mean', 'X26_mean', 'X50_mean', 'X3112_mean' ]\n",
    "    WANDB_INIT =  False\n",
    "    FOLD = 0 # Which fold to set as validation data\n",
    "    IMAGE_SIZE =128\n",
    "    TARGET_IMAGE_SIZE =  224\n",
    "    T_MAX =        9\n",
    "    torch.manual_seed(RANDOM_NUMBER)\n",
    "    INCLUDE_EXTRA_FEATURES = True\n",
    "    EXTRA_FEATURES_NORMALIZATION = \"standard_scalar\"  #\"min_max_normalization\"  #\n",
    "    WEIGHT_DECAY = 0.01\n",
    "    TABULAR_NN_OUTPUT  =   128 # 256\n",
    "    IMG_MODEL_NAME = \"dinoV2\" #\"efficientnet_v2\" # \n",
    "    IMG_FINED_TUNED_WEIGHT = None  #f'{BASE_DIR}/swinV2.pth'\n",
    "    Lower_Quantile = 0.005\n",
    "    Upper_Quantile = 0.985 #0.985\n",
    "    RECOMPUTE_IMAGE_EMBEDDINGS = True\n",
    "    # \n",
    "    EXTRA_COLOUMN = ['WORLDCLIM_BIO1_annual_mean_temperature',\n",
    "       'WORLDCLIM_BIO12_annual_precipitation',\n",
    "       'WORLDCLIM_BIO13.BIO14_delta_precipitation_of_wettest_and_dryest_month',\n",
    "       'WORLDCLIM_BIO15_precipitation_seasonality',\n",
    "       'WORLDCLIM_BIO4_temperature_seasonality',\n",
    "       'WORLDCLIM_BIO7_temperature_annual_range',\n",
    "       'SOIL_bdod_0.5cm_mean_0.01_deg',\n",
    "       'SOIL_bdod_100.200cm_mean_0.01_deg',\n",
    "       'SOIL_bdod_15.30cm_mean_0.01_deg',\n",
    "       'SOIL_bdod_30.60cm_mean_0.01_deg',\n",
    "       'SOIL_bdod_5.15cm_mean_0.01_deg',\n",
    "       'SOIL_bdod_60.100cm_mean_0.01_deg', 'SOIL_cec_0.5cm_mean_0.01_deg',\n",
    "       'SOIL_cec_100.200cm_mean_0.01_deg',\n",
    "       'SOIL_cec_15.30cm_mean_0.01_deg', 'SOIL_cec_30.60cm_mean_0.01_deg',\n",
    "       'SOIL_cec_5.15cm_mean_0.01_deg', 'SOIL_cec_60.100cm_mean_0.01_deg',\n",
    "       'SOIL_cfvo_0.5cm_mean_0.01_deg',\n",
    "       'SOIL_cfvo_100.200cm_mean_0.01_deg',\n",
    "       'SOIL_cfvo_15.30cm_mean_0.01_deg',\n",
    "       'SOIL_cfvo_30.60cm_mean_0.01_deg',\n",
    "       'SOIL_cfvo_5.15cm_mean_0.01_deg',\n",
    "       'SOIL_cfvo_60.100cm_mean_0.01_deg',\n",
    "       'SOIL_clay_0.5cm_mean_0.01_deg',\n",
    "       'SOIL_clay_100.200cm_mean_0.01_deg',\n",
    "       'SOIL_clay_15.30cm_mean_0.01_deg',\n",
    "       'SOIL_clay_30.60cm_mean_0.01_deg',\n",
    "       'SOIL_clay_5.15cm_mean_0.01_deg',\n",
    "       'SOIL_clay_60.100cm_mean_0.01_deg',\n",
    "       'SOIL_nitrogen_0.5cm_mean_0.01_deg',\n",
    "       'SOIL_nitrogen_100.200cm_mean_0.01_deg',\n",
    "       'SOIL_nitrogen_15.30cm_mean_0.01_deg',\n",
    "       'SOIL_nitrogen_30.60cm_mean_0.01_deg',\n",
    "       'SOIL_nitrogen_5.15cm_mean_0.01_deg',\n",
    "       'SOIL_nitrogen_60.100cm_mean_0.01_deg',\n",
    "       'SOIL_ocd_0.5cm_mean_0.01_deg', 'SOIL_ocd_100.200cm_mean_0.01_deg',\n",
    "       'SOIL_ocd_15.30cm_mean_0.01_deg', 'SOIL_ocd_30.60cm_mean_0.01_deg',\n",
    "       'SOIL_ocd_5.15cm_mean_0.01_deg', 'SOIL_ocd_60.100cm_mean_0.01_deg',\n",
    "       'SOIL_ocs_0.30cm_mean_0.01_deg', 'SOIL_phh2o_0.5cm_mean_0.01_deg',\n",
    "       'SOIL_phh2o_100.200cm_mean_0.01_deg',\n",
    "       'SOIL_phh2o_15.30cm_mean_0.01_deg',\n",
    "       'SOIL_phh2o_30.60cm_mean_0.01_deg',\n",
    "       'SOIL_phh2o_5.15cm_mean_0.01_deg',\n",
    "       'SOIL_phh2o_60.100cm_mean_0.01_deg',\n",
    "       'SOIL_sand_0.5cm_mean_0.01_deg',\n",
    "       'SOIL_sand_100.200cm_mean_0.01_deg',\n",
    "       'SOIL_sand_15.30cm_mean_0.01_deg',\n",
    "       'SOIL_sand_30.60cm_mean_0.01_deg',\n",
    "       'SOIL_sand_5.15cm_mean_0.01_deg',\n",
    "       'SOIL_sand_60.100cm_mean_0.01_deg',\n",
    "       'SOIL_silt_0.5cm_mean_0.01_deg',\n",
    "       'SOIL_silt_100.200cm_mean_0.01_deg',\n",
    "       'SOIL_silt_15.30cm_mean_0.01_deg',\n",
    "       'SOIL_silt_30.60cm_mean_0.01_deg',\n",
    "       'SOIL_silt_5.15cm_mean_0.01_deg',\n",
    "       'SOIL_silt_60.100cm_mean_0.01_deg', 'SOIL_soc_0.5cm_mean_0.01_deg',\n",
    "       'SOIL_soc_100.200cm_mean_0.01_deg',\n",
    "       'SOIL_soc_15.30cm_mean_0.01_deg', 'SOIL_soc_30.60cm_mean_0.01_deg',\n",
    "       'SOIL_soc_5.15cm_mean_0.01_deg', 'SOIL_soc_60.100cm_mean_0.01_deg',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_01_._month_m1',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_02_._month_m1',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_03_._month_m1',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_04_._month_m1',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_05_._month_m1',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_01_._month_m10',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_02_._month_m10',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_03_._month_m10',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_04_._month_m10',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_05_._month_m10',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_01_._month_m11',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_02_._month_m11',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_03_._month_m11',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_04_._month_m11',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_05_._month_m11',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_01_._month_m12',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_02_._month_m12',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_03_._month_m12',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_04_._month_m12',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_05_._month_m12',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_01_._month_m2',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_02_._month_m2',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_03_._month_m2',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_04_._month_m2',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_05_._month_m2',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_01_._month_m3',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_02_._month_m3',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_03_._month_m3',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_04_._month_m3',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_05_._month_m3',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_01_._month_m4',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_02_._month_m4',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_03_._month_m4',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_04_._month_m4',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_05_._month_m4',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_01_._month_m5',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_02_._month_m5',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_03_._month_m5',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_04_._month_m5',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_05_._month_m5',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_01_._month_m6',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_02_._month_m6',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_03_._month_m6',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_04_._month_m6',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_05_._month_m6',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_01_._month_m7',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_02_._month_m7',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_03_._month_m7',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_04_._month_m7',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_05_._month_m7',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_01_._month_m8',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_02_._month_m8',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_03_._month_m8',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_04_._month_m8',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_05_._month_m8',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_01_._month_m9',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_02_._month_m9',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_03_._month_m9',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_04_._month_m9',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_05_._month_m9',\n",
    "       'VOD_C_2002_2018_multiyear_mean_m01',\n",
    "       'VOD_C_2002_2018_multiyear_mean_m02',\n",
    "       'VOD_C_2002_2018_multiyear_mean_m03',\n",
    "       'VOD_C_2002_2018_multiyear_mean_m04',\n",
    "       'VOD_C_2002_2018_multiyear_mean_m05',\n",
    "       'VOD_C_2002_2018_multiyear_mean_m06',\n",
    "       'VOD_C_2002_2018_multiyear_mean_m07',\n",
    "       'VOD_C_2002_2018_multiyear_mean_m08',\n",
    "       'VOD_C_2002_2018_multiyear_mean_m09',\n",
    "       'VOD_C_2002_2018_multiyear_mean_m10',\n",
    "       'VOD_C_2002_2018_multiyear_mean_m11',\n",
    "       'VOD_C_2002_2018_multiyear_mean_m12',\n",
    "       'VOD_Ku_1987_2017_multiyear_mean_m01',\n",
    "       'VOD_Ku_1987_2017_multiyear_mean_m02',\n",
    "       'VOD_Ku_1987_2017_multiyear_mean_m03',\n",
    "       'VOD_Ku_1987_2017_multiyear_mean_m04',\n",
    "       'VOD_Ku_1987_2017_multiyear_mean_m05',\n",
    "       'VOD_Ku_1987_2017_multiyear_mean_m06',\n",
    "       'VOD_Ku_1987_2017_multiyear_mean_m07',\n",
    "       'VOD_Ku_1987_2017_multiyear_mean_m08',\n",
    "       'VOD_Ku_1987_2017_multiyear_mean_m09',\n",
    "       'VOD_Ku_1987_2017_multiyear_mean_m10',\n",
    "       'VOD_Ku_1987_2017_multiyear_mean_m11',\n",
    "       'VOD_Ku_1987_2017_multiyear_mean_m12',\n",
    "       'VOD_X_1997_2018_multiyear_mean_m01',\n",
    "       'VOD_X_1997_2018_multiyear_mean_m02',\n",
    "       'VOD_X_1997_2018_multiyear_mean_m03',\n",
    "       'VOD_X_1997_2018_multiyear_mean_m04',\n",
    "       'VOD_X_1997_2018_multiyear_mean_m05',\n",
    "       'VOD_X_1997_2018_multiyear_mean_m06',\n",
    "       'VOD_X_1997_2018_multiyear_mean_m07',\n",
    "       'VOD_X_1997_2018_multiyear_mean_m08',\n",
    "       'VOD_X_1997_2018_multiyear_mean_m09',\n",
    "       'VOD_X_1997_2018_multiyear_mean_m10',\n",
    "       'VOD_X_1997_2018_multiyear_mean_m11',\n",
    "       'VOD_X_1997_2018_multiyear_mean_m12'\n",
    "       ]\n",
    "\n",
    "    N_TARGETS  =len(TRAITS_NAME)  \n",
    "\n",
    "\n",
    "CONFIG = Config()\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "if CONFIG.WANDB_INIT:\n",
    "    wandb.login()\n",
    "    wandb.init(project=\"cs680v3\",group=\"dino_V2\",name=\"submission_dinoV2_no_fine_tune\",\n",
    "            config = {\n",
    "        \"LR_max\": CONFIG.LR_MAX,\n",
    "        \"WEIGHT_DECAY\":CONFIG.WEIGHT_DECAY,\n",
    "        \"train_batch\" : CONFIG.TRAIN_BATCH_SIZE\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JPEG Files Processing:\n",
      "JPEG Files Processing End\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %% [markdown]\n",
    "# # Preprocessing the Tabular Data And Image Transformation\n",
    "\n",
    "# %%\n",
    "# define_transformation for the tabular data\n",
    "log_tf_col = CONFIG.TRAITS_NAME\n",
    "scale_feature_col =  CONFIG.EXTRA_COLOUMN + CONFIG.TRAITS_NAME\n",
    "log_transform = ColumnTransformer(transformers= [\n",
    "    ('log' , FunctionTransformer( np.log10  , inverse_func=sp.special.exp10, validate=False, check_inverse = True ,feature_names_out='one-to-one') , log_tf_col)\n",
    "        ] , verbose_feature_names_out=False ,remainder= 'passthrough'\n",
    "          )\n",
    "log_transform.set_output(transform='pandas')\n",
    "\n",
    "std_scale =  ColumnTransformer(transformers=[('scale',StandardScaler() ,scale_feature_col )  ],\n",
    "                               verbose_feature_names_out=False,\n",
    "                               remainder='passthrough'\n",
    "                               )\n",
    "std_scale.set_output(transform='pandas')\n",
    "scaling_pipeline = Pipeline(steps=[   \n",
    "                        (\"log\" , log_transform),\n",
    "                        (\"std_scale\" , std_scale )])\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "# preparing the tabular data that has been given \n",
    "BASE_DIR = CONFIG.BASE_DIR\n",
    "Train_DF = pd.read_csv(BASE_DIR  +  '/train.csv')\n",
    "Test_DF =  pd.read_csv(BASE_DIR  +  '/test.csv')\n",
    "Test_DF[log_tf_col] =1\n",
    "\n",
    "train_df , val_df = train_test_split(Train_DF,test_size=CONFIG.TRAIN_VAL_SPLIT_SIZE,shuffle=True)\n",
    "for column in CONFIG.TRAITS_NAME:\n",
    "    lower_quantile = train_df[column].quantile(CONFIG.Lower_Quantile)\n",
    "    upper_quantile = train_df[column].quantile(CONFIG.Upper_Quantile)\n",
    "    train_df = train_df[(train_df[column] >= lower_quantile) & (train_df[column] <= upper_quantile)]\n",
    "\n",
    "# train_df = train_df[(np.abs(stats.zscore(np.log10(train_df[CONFIG.TRAITS_NAME]))<2).all(axis=1)  )]\n",
    "\n",
    "print('JPEG Files Processing:')\n",
    "train_df['file_path'] = train_df['id'].apply(lambda s: f'{BASE_DIR}/train_images/{s}.jpeg')\n",
    "train_df['jpeg_bytes'] = train_df['file_path'].apply(lambda fp: open(fp, 'rb').read())\n",
    "\n",
    "val_df['file_path'] = val_df['id'].apply(lambda s: f'{BASE_DIR}/train_images/{s}.jpeg')\n",
    "val_df['jpeg_bytes'] = val_df['file_path'].apply(lambda fp: open(fp, 'rb').read())\n",
    "\n",
    "Test_DF['file_path'] = Test_DF['id'].apply(lambda s: f'{BASE_DIR}/test_images/{s}.jpeg')\n",
    "Test_DF['jpeg_bytes'] = Test_DF['file_path'].apply(lambda fp: open(fp, 'rb').read())\n",
    "print('JPEG Files Processing End')    \n",
    "\n",
    "\n",
    "# train_tabular = train_df.drop(columns = ['id'] + CONFIG.TRAITS_NAME)\n",
    "# val_tabular =    val_df.drop(columns = ['id'] + CONFIG.TRAITS_NAME)\n",
    "# test_tabular = test.drop(columns = ['id'])\n",
    "\n",
    "# scaling of the training data !\n",
    "\n",
    "training_scaling_pipeline = Pipeline(steps=[   \n",
    "                        (\"log\" , log_transform),\n",
    "                        (\"std_scale\" , std_scale )])\n",
    "\n",
    "train_tabular_scaled =  training_scaling_pipeline.fit_transform(train_df)\n",
    "validation_tabular_scaled = training_scaling_pipeline.transform(val_df)\n",
    "\n",
    "test_tabular_scaled = training_scaling_pipeline.transform(Test_DF)\n",
    "Test_DF[log_tf_col]\n",
    "\"inverse\"\n",
    "# tt =   scaling_pipeline['std_scale']['scale'].inverse_transform(train_tabular_scaled[CONFIG.EXTRA_COLOUMN + CONFIG.TRAITS_NAME])\n",
    "# tt = pd.DataFrame(tt, columns=CONFIG.EXTRA_COLOUMN + CONFIG.TRAITS_NAME)\n",
    "# tt2 = scaling_pipeline['log']['log'].inverse_transform(tt[CONFIG.TRAITS_NAME])\n",
    "# tt2  = pd.concat([tt2 ,tt[CONFIG.EXTRA_COLOUMN]],axis=1)                 \n",
    "                  \n",
    "\n",
    "\n",
    "# some hyper parameter for lr scheduler\n",
    "CONFIG.N_TRAIN_SAMPLES = len(train_df)\n",
    "CONFIG.N_STEPS_PER_EPOCH = (CONFIG.N_TRAIN_SAMPLES // CONFIG.TRAIN_BATCH_SIZE)\n",
    "CONFIG.N_STEPS = CONFIG.N_STEPS_PER_EPOCH * CONFIG.NUM_EPOCHS + 1   \n",
    "\n",
    "# %%\n",
    "# Image PreProcessing\n",
    "\n",
    "MEAN = [0.485, 0.456, 0.406]\n",
    "STD = [0.229, 0.224, 0.225]\n",
    "TRAIN_TRANSFORMS = A.Compose([\n",
    "    A.Resize(CONFIG.TARGET_IMAGE_SIZE,CONFIG.TARGET_IMAGE_SIZE),\n",
    "    A.RandomResizedCrop(size=(CONFIG.TARGET_IMAGE_SIZE,CONFIG.TARGET_IMAGE_SIZE), interpolation=cv2.INTER_CUBIC,p=0.4),  # Simulate different crops\n",
    "    A.HorizontalFlip(p=0.2),  \n",
    "    A.ToFloat(),\n",
    "    A.Normalize(mean=MEAN, std=STD, max_pixel_value=1),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "TEST_TRANSFORMS = A.Compose([\n",
    "    A.Resize(CONFIG.TARGET_IMAGE_SIZE,CONFIG.TARGET_IMAGE_SIZE),\n",
    "    #A.CenterCrop(CONFIG.TARGET_IMAGE_SIZE,CONFIG.TARGET_IMAGE_SIZE),\n",
    "    A.ToFloat(),\n",
    "    A.Normalize(mean=MEAN, std=STD, max_pixel_value=1),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# # Creating the Dataloader for Train , Validation and Testing \n",
    "\n",
    "# %%\n",
    "class create_dataset(Dataset):\n",
    "    def __init__(self, X_jpeg_bytes, X_tabular, y, transforms=None):\n",
    "        self.X_jpeg_bytes = X_jpeg_bytes\n",
    "        self.X_tabular = X_tabular\n",
    "        self.y = y\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X_sample = self.transforms(\n",
    "            image=imageio.imread(self.X_jpeg_bytes[index]),\n",
    "        )['image']\n",
    "        X_tabular_sample = self.X_tabular[index]\n",
    "        y_sample = self.y[index]\n",
    "\n",
    "        return X_sample, X_tabular_sample, y_sample\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = create_dataset(\n",
    "    X_jpeg_bytes = train_tabular_scaled['jpeg_bytes'].values,\n",
    "    X_tabular = train_tabular_scaled[CONFIG.EXTRA_COLOUMN].values.astype(np.float32),\n",
    "    y = train_tabular_scaled[CONFIG.TRAITS_NAME].values.astype(np.float32),\n",
    "    transforms= TRAIN_TRANSFORMS\n",
    ")\n",
    "\n",
    "validation_dataset = create_dataset(\n",
    "    X_jpeg_bytes = validation_tabular_scaled['jpeg_bytes'].values,\n",
    "    X_tabular = validation_tabular_scaled[CONFIG.EXTRA_COLOUMN].values.astype(np.float32),\n",
    "    y = validation_tabular_scaled[CONFIG.TRAITS_NAME].values.astype(np.float32),\n",
    "    transforms= TEST_TRANSFORMS\n",
    ")\n",
    "\n",
    "test_dataset = create_dataset(\n",
    "    X_jpeg_bytes = test_tabular_scaled['jpeg_bytes'].values,\n",
    "    X_tabular = test_tabular_scaled[CONFIG.EXTRA_COLOUMN].values.astype(np.float32),\n",
    "    y = test_tabular_scaled[\"id\"].values,\n",
    "    transforms= TEST_TRANSFORMS\n",
    "    )\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CONFIG.TRAIN_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=0#psutil.cpu_count(),\n",
    ")\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "    validation_dataset,\n",
    "    batch_size=CONFIG.VAL_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers= 0 #psutil.cpu_count(),\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=CONFIG.TEST_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers= 0 #psutil.cpu_count(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/prajwal/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "/home/prajwal/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/home/prajwal/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/home/prajwal/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n",
      "INFO:dinov2:using SwiGLU layer as FFN\n"
     ]
    }
   ],
   "source": [
    "if CONFIG.RECOMPUTE_IMAGE_EMBEDDINGS:\n",
    "    class ImageBackbone_dinoV2(nn.Module):\n",
    "        def __init__(self, backbone_name, weight_path, out_features, fixed_feature_extractor=False):\n",
    "            super().__init__()\n",
    "            self.out_features = out_features\n",
    "            self.backbone = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitg14_reg').to(DEVICE)\n",
    "            in_features = self.backbone.num_features\n",
    "            self.backbone.head = nn.Identity()\n",
    "            self.head = nn.Sequential(\n",
    "                nn.Linear(in_features, out_features),\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.backbone(x)\n",
    "            return self.head(x)\n",
    "\n",
    "    class TabularBackbone(nn.Module):\n",
    "        def __init__(self, n_features, out_features):\n",
    "            super().__init__()\n",
    "            self.out_features = out_features\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Linear(n_features, 512),\n",
    "                nn.BatchNorm1d(512),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(0.1),\n",
    "                nn.Linear(512, out_features),\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.fc(x)    \n",
    "\n",
    "\n",
    "\n",
    "    # %% [markdown]\n",
    "    # # Combine the image feature extraction model with tabular feature extraction model \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #backbone_name, weight_path, out_features, fixed_feature_extractor=False\n",
    "    # %%\n",
    "    class CustomModel(nn.Module):\n",
    "        def __init__(self,model_name , fine_tuned_img_weights , img_out_features , fixed_feature_extractor , traits_target_feature):\n",
    "            super().__init__()\n",
    "            self.img_backbone =  ImageBackbone_dinoV2(backbone_name = model_name,weight_path = fine_tuned_img_weights ,out_features =  img_out_features ,fixed_feature_extractor=fixed_feature_extractor)\n",
    "            self.extra_features_model = TabularBackbone(n_features  = len(CONFIG.EXTRA_COLOUMN) , out_features=128)\n",
    "            self.fc = nn.Sequential(\n",
    "                nn.Linear(self.extra_features_model.out_features + self.img_backbone.out_features,1024 ),  # 256\n",
    "                nn.BatchNorm1d(1024),\n",
    "                nn.GELU(),\n",
    "                # nn.Dropout(0.1),\n",
    "                nn.Linear(1024, 256),\n",
    "                nn.BatchNorm1d(256),\n",
    "                nn.GELU(),\n",
    "                # nn.Dropout(0.1),\n",
    "                nn.Linear(256, traits_target_feature),\n",
    "            )        \n",
    "        def forward(self,image,x):\n",
    "            output_image = self.img_backbone(image) \n",
    "            z = self.extra_features_model(x) \n",
    "            inputs  = torch.cat((output_image,z), 1 )\n",
    "            output = self.fc(inputs)\n",
    "            return output\n",
    "\n",
    "    # input_channels = len(CONFIG.EXTRA_COLOUMN) ,out_channels =CONFIG.TABULAR_NN_OUTPUT, target_features_num= len(CONFIG.TRAITS_NAME), tim_num_class=CONFIG.TIM_NUM_CLASS , model_name=CONFIG.TIM_MODEL_NAME\n",
    "    # %%\n",
    "    model = CustomModel(model_name=CONFIG.IMG_MODEL_NAME , fine_tuned_img_weights= CONFIG.IMG_FINED_TUNED_WEIGHT,img_out_features = CONFIG.IMG_OUT_FEATURES, fixed_feature_extractor=True,traits_target_feature = CONFIG.N_TARGETS )\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_68537/1428918084.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('/home/prajwal/cs680/cs680_kaggle/data/submission_dinoV2_not_fine_tuned.pth'))\n"
     ]
    }
   ],
   "source": [
    "if CONFIG.RECOMPUTE_IMAGE_EMBEDDINGS:\n",
    "    model.load_state_dict(torch.load('/home/prajwal/cs680/cs680_kaggle/data/submission_dinoV2_not_fine_tuned.pth'))\n",
    "    model.eval()\n",
    "    model.to(DEVICE) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_img_test , X_features,targets= next(iter(train_dataloader))\n",
    "# x_f = X_features.to(DEVICE);\n",
    "# x_img =X_img_test.to(DEVICE);\n",
    "# output = model(x_img,x_f)\n",
    "# activation['fc.3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x7db915516d50>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "model.fc[3].register_forward_hook((get_activation('fc.3')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# catbooster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_embeddings_dinoV2(batch_image , batch_features,batch_size= CONFIG.TRAIN_BATCH_SIZE):\n",
    "    output = model(batch_image,batch_features);\n",
    "    curr_feature_embeddings=activation['fc.3']\n",
    "    feature_embeddings = (curr_feature_embeddings.cpu().numpy())\n",
    "    return feature_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "in training Batch: 19015/19016 :   0%|          | 0/19016 [36:48<?, ?it/s]\n",
      "IN val Batch: 216/217 :   0%|          | 0/217 [00:25<?, ?it/s]\n",
      "IN test Batch: 3195/3196 :   0%|          | 0/3196 [06:11<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "SAVE_DIR = \"/home/prajwal/cs680/cs680_kaggle/catboost_dinoV2_features/\"\n",
    "suffix = 'dinoV2_embs_reg'\n",
    "if CONFIG.RECOMPUTE_IMAGE_EMBEDDINGS:\n",
    "    train_feat_embeddings_list = []\n",
    "    val_feat_embeddings_list   = []\n",
    "    test_feat_embeddings_list =  []\n",
    "    with tqdm.tqdm(total=len(train_dataloader))as trainingLoop  :\n",
    "        for index,batch in enumerate(iter(train_dataloader)) :\n",
    "            X_train_img = batch[0].to(DEVICE) \n",
    "            X_train_feat  = batch[1].to(DEVICE)\n",
    "            X_train_id  = batch[2].to(DEVICE)\n",
    "            train_image_embeddings = get_image_embeddings_dinoV2(X_train_img, X_train_feat)\n",
    "            trainingLoop.set_description(f\"in training Batch: {index}/{len(train_dataloader)} \")\n",
    "            train_feat_embeddings_list.extend(train_image_embeddings)\n",
    "        np.save(f'{SAVE_DIR}/train_{suffix}', np.array(train_feat_embeddings_list))\n",
    "        \n",
    "    with tqdm.tqdm(total=len(validation_dataloader)) as valloop  :\n",
    "        for index,batch in enumerate(iter(validation_dataloader)) :\n",
    "            X_train_img = batch[0].to(DEVICE) \n",
    "            X_train_feat  = batch[1].to(DEVICE)\n",
    "            X_train_id  = batch[2].to(DEVICE)\n",
    "            val_image_embeddings = get_image_embeddings_dinoV2(X_train_img, X_train_feat)\n",
    "            valloop.set_description(f\"IN val Batch: {index}/{len(validation_dataloader)} \")\n",
    "            val_feat_embeddings_list.extend(val_image_embeddings)\n",
    "        np.save(f'{SAVE_DIR}/val_{suffix}', np.array(val_feat_embeddings_list))\n",
    "    \n",
    "    with tqdm.tqdm(total=len(test_dataloader)) as testloop :\n",
    "        for index,batch in enumerate(iter(test_dataloader)) :\n",
    "            X_train_img = batch[0].to(DEVICE) \n",
    "            X_train_feat  = batch[1].to(DEVICE)\n",
    "            X_train_id  = batch[2].to(DEVICE)\n",
    "            test_image_embeddings = get_image_embeddings_dinoV2(X_train_img, X_train_feat)\n",
    "            testloop.set_description(f\"IN test Batch: {index}/{len(test_dataloader)} \")\n",
    "            test_feat_embeddings_list.extend(test_image_embeddings)\n",
    "        np.save(f'{SAVE_DIR}/test_{suffix}', np.array(test_feat_embeddings_list))       \n",
    "else:\n",
    "    suffix = 'image_embs_dinov2_vitg14_reg'\n",
    "    train_image_embeddings = np.load(f'{SAVE_DIR}/train_{suffix}.npy')\n",
    "    val_image_embeddings = np.load(f'{SAVE_DIR}/val_{suffix}.npy')\n",
    "    test_image_embeddings = np.load(f'{SAVE_DIR}/test_{suffix}.npy')\n",
    "    print(f'Embeddings {suffix} loaded from dataset.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6391"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from catboost import Pool, CatBoostRegressor\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tabular = train_tabular_scaled[CONFIG.EXTRA_COLOUMN].values.astype(np.float32)\n",
    "y_train_tabular = train_tabular_scaled[CONFIG.TRAITS_NAME].values.astype(np.float32)\n",
    "\n",
    "X_val_tabular = validation_tabular_scaled[CONFIG.EXTRA_COLOUMN].values.astype(np.float32)\n",
    "y_val_tabular = validation_tabular_scaled[CONFIG.TRAITS_NAME].values.astype(np.float32)\n",
    "\n",
    "X_test_tabular = test_tabular_scaled[CONFIG.EXTRA_COLOUMN].values.astype(np.float32)\n",
    "y_test_tabular = test_tabular_scaled[\"id\"].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = PolynomialFeatures(2).fit_transform(X_train_tabular)[:, :first_n_poly_feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38032, 1000)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feat_embeddings_array = np.empty((0,256))\n",
    "val_feat_embeddings_array = np.empty((0,256))\n",
    "test_feat_embeddings_array = np.empty((0,256))\n",
    "for i in range(len(train_feat_embeddings_list)):\n",
    "    #print(i)\n",
    "    train_feat_embeddings_array= np.vstack((train_feat_embeddings_array ,train_feat_embeddings_list[i] ))\n",
    "    \n",
    "for i in range(len(val_feat_embeddings_list)):\n",
    "    #print(i)\n",
    "    val_feat_embeddings_array= np.vstack((val_feat_embeddings_array ,val_feat_embeddings_list[i] ))\n",
    "    \n",
    "for i in range(len(test_feat_embeddings_list)):\n",
    "    #print(i)\n",
    "    test_feat_embeddings_array= np.vstack((test_feat_embeddings_array ,test_feat_embeddings_list[i] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'{SAVE_DIR}/train_{suffix}_array', np.array(train_feat_embeddings_array))\n",
    "np.save(f'{SAVE_DIR}/val_{suffix}_array', np.array(val_feat_embeddings_array))\n",
    "np.save(f'{SAVE_DIR}/test_{suffix}_array', np.array(test_feat_embeddings_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can potentially use all the polynomial features but it would take an etenriny to train the models\n",
    "first_n_poly_feats = 1000\n",
    "\n",
    "#drop images and id\n",
    "train_features_mask_all = np.concatenate(\n",
    "    (PolynomialFeatures(2).fit_transform(X_train_tabular)[:, :first_n_poly_feats], train_feat_embeddings_array), axis=1\n",
    ")\n",
    "val_features_mask_all = np.concatenate(\n",
    "    (PolynomialFeatures(2).fit_transform(X_val_tabular)[:, :first_n_poly_feats], val_feat_embeddings_array), axis=1\n",
    ")\n",
    "test_features_all = np.concatenate(\n",
    "    (PolynomialFeatures(2).fit_transform(X_test_tabular)[:, :first_n_poly_feats], test_feat_embeddings_array), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([    0,     1,     2,     3,     4,     5,     6,     7,     8,     9,\n",
       "       ...\n",
       "        1247,  1248,  1249,  1250,  1251,  1252,  1253,  1254,  1255, 'emb'],\n",
       "      dtype='object', length=1257)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_mask_df = pd.DataFrame(train_features_mask_all)\n",
    "train_features_mask_df['emb'] = list(train_feat_embeddings_list)\n",
    "\n",
    "val_features_mask_df = pd.DataFrame(val_features_mask_all)\n",
    "val_features_mask_df['emb'] = list(val_feat_embeddings_list)\n",
    "\n",
    "test_features_mask_df = pd.DataFrame(test_features_all)\n",
    "test_features_mask_df['emb'] = list(test_feat_embeddings_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(434, 6)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_tabular.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2247ed2988cc43618e26f6945e4b6a60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = {}\n",
    "scores = {}\n",
    "models = {}\n",
    "scores = {}\n",
    "for i, col in tqdm(enumerate(CONFIG.TRAITS_NAME), total=len(CONFIG.TRAITS_NAME)):\n",
    "    y_curr = y_train_tabular[:, i]\n",
    "    y_curr_val = y_val_tabular[:, i]\n",
    "    train_pool = Pool(train_features_mask_df, y_curr, embedding_features=['emb'])\n",
    "    val_pool = Pool(val_features_mask_df, y_curr_val, embedding_features=['emb'])\n",
    "    \n",
    "    # tried to tune these parameters but without real success \n",
    "    model = CatBoostRegressor(iterations=1500, learning_rate=0.06, loss_function='RMSE', verbose=1, random_state=CONFIG.RANDOM_NUMBER)\n",
    "    model.fit(train_pool)\n",
    "    models[col] = model\n",
    "    \n",
    "    y_curr_val_pred = model.predict(val_pool)\n",
    "    \n",
    "    r2_col = r2_score(y_curr_val, y_curr_val_pred)\n",
    "    scores[col] = r2_col\n",
    "    print(f'Target: {col}, R2: {r2_col:.3f}')\n",
    "# this val score somewhat correlates with submission score bit I didn't really bother\n",
    "print(f'Mean R2: {np.mean(list(scores.values())):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'id': test_features_mask_df['id']})\n",
    "submission[CONFIG.TRAITS_NAME] = 0\n",
    "submission.columns = submission.columns.str.replace('_mean', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, col in enumerate(CONFIG.TARGET_COLUMNS):\n",
    "    test_pool = Pool(test_features_mask_df, embedding_features=['emb'])\n",
    "    col_pred = models[col].predict(test_pool)\n",
    "    submission[col.replace('_mean', '')] = col_pred\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # to save model \n",
    "# class BestModelSaveCallback:\n",
    "#     def __init__(self, save_path):\n",
    "#         self.save_path = save_path\n",
    "#         self.best_accuracy = -1\n",
    "\n",
    "#     def __call__(self, accuracy,model):\n",
    "#         if accuracy > self.best_accuracy:\n",
    "#             self.best_accuracy = accuracy\n",
    "#             model.to(device = \"cpu\")\n",
    "#             torch.save(model.state_dict(), self.save_path)\n",
    "#             model.to(device=DEVICE)\n",
    "            \n",
    "#             print(\"generating result on test data\")\n",
    "#             submission_df = pd.DataFrame(columns=CONFIG.TRAITS_NAME)\n",
    "#             for index , batch in tqdm.tqdm(enumerate(iter(test_dataloader))):\n",
    "#                 X_img_test = batch[0] \n",
    "#                 X_features  = batch[1]\n",
    "#                 test_id  = batch[2]\n",
    "#                 #print(batch) \n",
    "#                 with torch.no_grad():\n",
    "#                     #print(X_img_test.shape, X_features.shape)\n",
    "#                     y_pred = model(X_img_test.to(DEVICE),X_features.to(DEVICE)).detach().cpu().numpy()  #,X_features.to(DEVICE)\n",
    "                \n",
    "#                     pred_pd = pd.DataFrame(columns=CONFIG.EXTRA_COLOUMN + CONFIG.TRAITS_NAME)\n",
    "#                     pred_pd[CONFIG.EXTRA_COLOUMN] =-1\n",
    "#                     pred_pd[CONFIG.TRAITS_NAME] = y_pred \n",
    "\n",
    "#                     temp1 =   scaling_pipeline['std_scale']['scale'].inverse_transform(pred_pd)\n",
    "#                     temp2=    pd.DataFrame(temp1, columns=CONFIG.EXTRA_COLOUMN + CONFIG.TRAITS_NAME)\n",
    "#                     pred_final =   scaling_pipeline['log']['log'].inverse_transform(temp2[CONFIG.TRAITS_NAME])\n",
    "#                     #pred_final[\"id\"] = test_id.cpu().detach().numpy()\n",
    "#                     submission_df = pd.concat([submission_df, pred_final.assign(id=test_id.cpu().detach().numpy())], ignore_index=True)\n",
    "#             # submission_df.to_csv('submission_self_tuning.csv', index=False)\n",
    "#             submission_df[[\"id\"]  + CONFIG.TRAITS_NAME ].to_csv('submission_dinoV2_not_fine_tuned.csv', index=False)\n",
    "#             print(\"Submit!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
