{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import imageio.v3 as imageio\n",
    "import albumentations as A\n",
    "\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import Dataset\n",
    "from torch import nn\n",
    "import torchvision.transforms.functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "import joblib\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "DATA_DIRECTORY = os.path.join(os.getcwd(),\"data\")\n",
    "torch.cuda.empty_cache()\n",
    "# torch.cuda.set_per_process_memory_fraction(0.99, device=0)\n",
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"caching_allocator\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    IMAGE_SIZE = 384\n",
    "    TARGET_COLUMNS = ['X4_mean', 'X11_mean', 'X18_mean', 'X50_mean', 'X26_mean', 'X3112_mean']\n",
    "    N_TARGETS = len(TARGET_COLUMNS)\n",
    "    BATCH_SIZE = 10\n",
    "    LR_MAX = 1e-4\n",
    "    WEIGHT_DECAY = 0.01\n",
    "    N_EPOCHS = 6\n",
    "    TRAIN_MODEL = True\n",
    "    IS_INTERACTIVE =False # os.environ['KAGGLE_KERNEL_RUN_TYPE'] == 'Interactive'\n",
    "        \n",
    "CONFIG = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path =  os.path.join(os.getcwd() , 'data')\n",
    "imgs_train = f\"{path}/train_images\"\n",
    "imgs_test = f\"{path}/test_images\"\n",
    "\n",
    "train = pd.read_csv(f\"{path}/train.csv\")\n",
    "train['file_path'] = train['id'].apply(lambda s: f'{path}/train_images/{s}.jpeg')\n",
    "train['jpeg_bytes'] = train['file_path'].apply(lambda fp: open(fp, 'rb').read())\n",
    "train.to_pickle('train.pkl')\n",
    "\n",
    "for column in CONFIG.TARGET_COLUMNS:\n",
    "    lower_quantile = train[column].quantile(0.005)\n",
    "    upper_quantile = train[column].quantile(0.985)  \n",
    "    train = train[(train[column] >= lower_quantile) & (train[column] <= upper_quantile)]\n",
    "\n",
    "CONFIG.N_TRAIN_SAMPLES = len(train)\n",
    "CONFIG.N_STEPS_PER_EPOCH = (CONFIG.N_TRAIN_SAMPLES // CONFIG.BATCH_SIZE)\n",
    "CONFIG.N_STEPS = CONFIG.N_STEPS_PER_EPOCH * CONFIG.N_EPOCHS + 1\n",
    "\n",
    "test = pd.read_csv(f\"{path}/test.csv\")\n",
    "test['file_path'] = test['id'].apply(lambda s: f'{path}/test_images/{s}.jpeg')\n",
    "test['jpeg_bytes'] = test['file_path'].apply(lambda fp: open(fp, 'rb').read())\n",
    "test.to_pickle('test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_FEATURES = ['X4_mean', 'X11_mean', 'X18_mean', 'X50_mean', 'X26_mean', 'X3112_mean']\n",
    "\n",
    "y_train = np.zeros_like(train[CONFIG.TARGET_COLUMNS], dtype=np.float32)\n",
    "for target_idx, target in enumerate(CONFIG.TARGET_COLUMNS):\n",
    "    v = train[target].values\n",
    "    if target in LOG_FEATURES:\n",
    "        v = np.log10(v)\n",
    "    y_train[:, target_idx] = v\n",
    "\n",
    "SCALER = StandardScaler()\n",
    "y_train = SCALER.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEAN = np.array([0.485, 0.456, 0.406])\n",
    "STD = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "TEST_TRANSFORMS = A.Compose([\n",
    "        A.Resize(CONFIG.IMAGE_SIZE, CONFIG.IMAGE_SIZE),\n",
    "        A.ToFloat(),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=1),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, X_jpeg_bytes, y, transforms=None):\n",
    "        self.X_jpeg_bytes = X_jpeg_bytes\n",
    "        self.y = y\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_jpeg_bytes)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X_sample = self.transforms(\n",
    "            image=imageio.imread(self.X_jpeg_bytes[index]),\n",
    "        )['image']\n",
    "        y_sample = self.y[index]\n",
    "        \n",
    "        return X_sample, y_sample\n",
    "\n",
    "test_dataset = Dataset(\n",
    "    test['jpeg_bytes'].values,\n",
    "    test['id'].values,\n",
    "    TEST_TRANSFORMS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_79770/1456432996.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.backbone = torch.load(f'{BASE_DIR}/model_08_ensemble.pth', map_location='cuda')\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = os.path.join(os.getcwd() , 'data')\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = torch.load(f'{BASE_DIR}/model_08_ensemble.pth', map_location='cuda')\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        return self.backbone(inputs)\n",
    "\n",
    "model = Model()\n",
    "model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tta_augmentations(image):\n",
    "    aug_images = []\n",
    "    # Original image\n",
    "    aug_images.append(image)\n",
    "    # Flipped images\n",
    "    aug_images.append(F.vflip(image))  # Vertical flip\n",
    "    aug_images.append(F.hflip(image))  # Horizontal flip\n",
    "    # Rotated images\n",
    "    angle = random.uniform(-30, 30)  # Random rotation angle between -30 and 30 degrees\n",
    "    rotated_image = F.rotate(image, angle)  \n",
    "    aug_images.append(rotated_image)\n",
    "    return aug_images\n",
    "\n",
    "def tta_predict(model, image, device):\n",
    "    augmented_images = tta_augmentations(image)\n",
    "    weights = torch.ones(len(augmented_images), dtype=torch.float32, device=device) / len(augmented_images)\n",
    "\n",
    "    final_prediction = None\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, aug_image in enumerate(augmented_images):\n",
    "            aug_image = aug_image.unsqueeze(0).to(device)\n",
    "            prediction = model(aug_image)\n",
    "            if final_prediction is None:\n",
    "                final_prediction = weights[i] * prediction\n",
    "            else:\n",
    "                final_prediction += weights[i] * prediction\n",
    "\n",
    "    return final_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9315dcc5c83f40068caf43fcd19e5fa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "'collections.OrderedDict' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X_sample_test, test_id \u001b[38;5;129;01min\u001b[39;00m tqdm(test_dataset):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 7\u001b[0m         prediction \u001b[38;5;241m=\u001b[39m tta_predict(model, X_sample_test, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m      9\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m SCALER\u001b[38;5;241m.\u001b[39minverse_transform([prediction])[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     10\u001b[0m     row \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m: test_id}\n",
      "Cell \u001b[0;32mIn[21], line 23\u001b[0m, in \u001b[0;36mtta_predict\u001b[0;34m(model, image, device)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, aug_image \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(augmented_images):\n\u001b[1;32m     22\u001b[0m     aug_image \u001b[38;5;241m=\u001b[39m aug_image\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 23\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m model(aug_image)\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m final_prediction \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     25\u001b[0m         final_prediction \u001b[38;5;241m=\u001b[39m weights[i] \u001b[38;5;241m*\u001b[39m prediction\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_gpu/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_gpu/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[20], line 8\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone(inputs)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'collections.OrderedDict' object is not callable"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "SUBMISSION_ROWS = []\n",
    "model.eval()\n",
    "\n",
    "for X_sample_test, test_id in tqdm(test_dataset):\n",
    "    with torch.no_grad():\n",
    "        prediction = tta_predict(model, X_sample_test, 'cuda').detach().cpu().numpy().squeeze()\n",
    "\n",
    "    y_pred = SCALER.inverse_transform([prediction])[0]\n",
    "    row = {'id': test_id}\n",
    "\n",
    "    for k, v in zip(CONFIG.TARGET_COLUMNS, y_pred):\n",
    "        if k in LOG_FEATURES:\n",
    "            row[k.replace('_mean', '')] = 10 ** v\n",
    "        else:\n",
    "            row[k.replace('_mean', '')] = v\n",
    "\n",
    "    SUBMISSION_ROWS.append(row)\n",
    "\n",
    "submission_eda = pd.DataFrame(SUBMISSION_ROWS).copy()\n",
    "\n",
    "submission_eda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import imageio.v3 as imageio\n",
    "import albumentations as A\n",
    "\n",
    "# import torch_xla as xla\n",
    "# import torch_xla.core.xla_model as xm\n",
    "# import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "# import torch_xla.distributed.xla_backend\n",
    "\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch import nn\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.spatial.distance import squareform\n",
    "\n",
    "\n",
    "import torch\n",
    "import timm\n",
    "import torchmetrics\n",
    "import time\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    IMAGE_SIZE = 384\n",
    "    BACKBONE = 'swin_large_patch4_window12_384.ms_in22k_ft_in1k'\n",
    "    TARGET_COLUMNS = ['X4_mean', 'X11_mean', 'X18_mean', 'X50_mean', 'X26_mean', 'X3112_mean']\n",
    "    N_TARGETS = len(TARGET_COLUMNS)\n",
    "    BATCH_SIZE = 64\n",
    "    LR_MAX = 1e-4\n",
    "    WEIGHT_DECAY = 0.01\n",
    "    N_EPOCHS = 5\n",
    "    TRAIN_MODEL = True\n",
    "    IS_INTERACTIVE = True  # os.environ['KAGGLE_KERNEL_RUN_TYPE'] == 'Interactive'\n",
    "    tpu_ids = range(8)\n",
    "    Lower_Quantile = 0.005\n",
    "    Upper_Quantile = 0.985\n",
    "    SHRINK_SAMPLES = True\n",
    "\n",
    "CONFIG = Config()\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, X_jpeg_bytes, X_tabular, y, transforms=None):\n",
    "        self.X_jpeg_bytes = X_jpeg_bytes\n",
    "        self.X_tabular = X_tabular\n",
    "        self.y = y\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X_sample = self.transforms(\n",
    "            image=imageio.imread(self.X_jpeg_bytes[index]),\n",
    "        )['image']\n",
    "        X_tabular_sample = self.X_tabular[index]\n",
    "        y_sample = self.y[index]\n",
    "\n",
    "        return X_sample, X_tabular_sample, y_sample\n",
    "    \n",
    "    \n",
    "class TabularBackbone(nn.Module):\n",
    "    def __init__(self, n_features, out_features):\n",
    "        super().__init__()\n",
    "        self.out_features = out_features\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(n_features, 512),\n",
    "            \n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.GELU(),\n",
    "            # nn.Dropout(0.1),\n",
    "            nn.Linear(512, out_features),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "    \n",
    "class ImageBackbone(nn.Module):\n",
    "    def __init__(self, backbone_name, weight_path, out_features, fixed_feature_extractor=False):\n",
    "        super().__init__()\n",
    "        self.out_features = out_features\n",
    "        self.backbone = timm.create_model(backbone_name, pretrained=False, num_classes=CONFIG.N_TARGETS)\n",
    "        self.backbone.load_state_dict(torch.load(weight_path))\n",
    "        if fixed_feature_extractor:\n",
    "            for param in self.backbone.parameters():\n",
    "                param.requires_grad = False\n",
    "        in_features = self.backbone.num_features\n",
    "        \n",
    "        self.backbone.head = nn.Identity()\n",
    "        self.head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features, out_features),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        return self.head(x)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, img_backbone, tab_backbone, out_features:int):\n",
    "        super().__init__()\n",
    "        self.img_backbone = img_backbone\n",
    "        self.tab_backbone = tab_backbone\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.tab_backbone.out_features + self.img_backbone.out_features, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.GELU(),\n",
    "            # nn.Dropout(0.1),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.GELU(),\n",
    "            # nn.Dropout(0.1),\n",
    "            nn.Linear(256, out_features),\n",
    "        )\n",
    "\n",
    "    def forward(self, img, tab):\n",
    "        img_features = self.img_backbone(img)\n",
    "        tab_features = self.tab_backbone(tab)\n",
    "        features = torch.cat([img_features, tab_features], dim=1)\n",
    "        return self.fc(features)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_79770/3158310507.py:60: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.backbone.load_state_dict(torch.load(weight_path))\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/input/swin-transformer-v1-planttraits2024-finetuned/pytorch/log3-noval-8epoch/1/model_08.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m img_backbone \u001b[38;5;241m=\u001b[39m ImageBackbone(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mswin_large_patch4_window12_384.ms_in22k_ft_in1k\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/input/swin-transformer-v1-planttraits2024-finetuned/pytorch/log3-noval-8epoch/1/model_08.pth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m384\u001b[39m, fixed_feature_extractor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m tab_backbone \u001b[38;5;241m=\u001b[39m TabularBackbone(n_features\u001b[38;5;241m=\u001b[39mtabular_scaled\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], out_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m)\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m Model(img_backbone, tab_backbone, CONFIG\u001b[38;5;241m.\u001b[39mN_TARGETS)\n",
      "Cell \u001b[0;32mIn[27], line 60\u001b[0m, in \u001b[0;36mImageBackbone.__init__\u001b[0;34m(self, backbone_name, weight_path, out_features, fixed_feature_extractor)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_features \u001b[38;5;241m=\u001b[39m out_features\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone \u001b[38;5;241m=\u001b[39m timm\u001b[38;5;241m.\u001b[39mcreate_model(backbone_name, pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_classes\u001b[38;5;241m=\u001b[39mCONFIG\u001b[38;5;241m.\u001b[39mN_TARGETS)\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(weight_path))\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fixed_feature_extractor:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone\u001b[38;5;241m.\u001b[39mparameters():\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_gpu/lib/python3.12/site-packages/torch/serialization.py:1065\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1063\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m-> 1065\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_like(f, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m   1066\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1067\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_gpu/lib/python3.12/site-packages/torch/serialization.py:468\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 468\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    470\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_gpu/lib/python3.12/site-packages/torch/serialization.py:449\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mopen\u001b[39m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/swin-transformer-v1-planttraits2024-finetuned/pytorch/log3-noval-8epoch/1/model_08.pth'"
     ]
    }
   ],
   "source": [
    "img_backbone = ImageBackbone('swin_large_patch4_window12_384.ms_in22k_ft_in1k', f'{BASE_DIR}/model_08_ensemble.pth', 384, fixed_feature_extractor=True)\n",
    "tab_backbone = TabularBackbone(n_features=tabular_scaled.shape[1], out_features=128)\n",
    "\n",
    "model = Model(img_backbone, tab_backbone, CONFIG.N_TARGETS)\n",
    "model = model.to(DEVICE)\n",
    "state_dict = torch.load(f'{BASE_DIR}/model_08_ensemble.pth')\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b331e6a0ed764a15ac165f1916d7a565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/models/riverfog7/swin-transformer-v1-planttraits2024-finetuned/pyTorch/log3-noval-8epoch/1/download/model_08.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 745M/745M [00:14<00:00, 52.1MB/s]\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.model_download(\"riverfog7/swin-transformer-v1-planttraits2024-finetuned/pyTorch/log3-noval-8epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/prajwal/.cache/kagglehub/models/riverfog7/swin-transformer-v1-planttraits2024-finetuned/pyTorch/log3-noval-8epoch/1'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
