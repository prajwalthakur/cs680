{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.13 (you have 1.4.10). Upgrade using: pip install --upgrade albumentations\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 \n",
    "import IPython\n",
    "from glob import glob\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tqdm\n",
    "#import seaborn as sns\n",
    "import albumentations as A\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "import torcheval \n",
    "import wandb\n",
    "import torchvision\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams.update(**{'figure.dpi':150})\n",
    "import psutil\n",
    "import imageio.v3 as imageio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, FunctionTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler , MinMaxScaler\n",
    "import scipy as sp\n",
    "# %%\n",
    "RANDOM_NUMBER = 42\n",
    "torch.manual_seed(RANDOM_NUMBER)\n",
    "\n",
    "# %% [markdown]\n",
    "# # select Device\n",
    "\n",
    "# %%\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else  \"cpu\") #\"cuda:1\" if torch.cuda.is_available() else \n",
    "# torch.cuda.set_per_process_memory_fraction(0.95, device=DEVICE)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "# %%\n",
    "class Config():\n",
    "\n",
    "    BASE_DIR = os.path.join(os.getcwd() , 'data')\n",
    "    train_df = pd.read_csv(BASE_DIR  +  '/train.csv')\n",
    "    TRAIN_VAL_SPLIT_SIZE = 0.14\n",
    "    TRAIN_BATCH_SIZE = 256\n",
    "    VAL_BATCH_SIZE = 128\n",
    "    TEST_BATCH_SIZE =128\n",
    "    LR_MAX = 1e-4 \n",
    "    NUM_EPOCHS = 20\n",
    "    IMG_OUT_FEATURES = 1024  #512  # swin     #768<-vit   \n",
    "    NORMALIZE_TARGET = \"log_transform_mean_std\"   #\"log_transform\" #\n",
    "    RANDOM_NUMBER = 42\n",
    "    NUM_FLODS  = 5\n",
    "    NUM_CLASSES = 6\n",
    "    TRAITS_NAME = ['X4_mean', 'X11_mean', 'X18_mean', 'X26_mean', 'X50_mean', 'X3112_mean' ]\n",
    "    WANDB_INIT =  True\n",
    "    FOLD = 0 # Which fold to set as validation data\n",
    "    IMAGE_SIZE =128\n",
    "    TARGET_IMAGE_SIZE =  256\n",
    "    T_MAX =        9\n",
    "    torch.manual_seed(RANDOM_NUMBER)\n",
    "    INCLUDE_EXTRA_FEATURES = True\n",
    "    EXTRA_FEATURES_NORMALIZATION = \"standard_scalar\"  #\"min_max_normalization\"  #\n",
    "    WEIGHT_DECAY = 0.01\n",
    "    TABULAR_NN_OUTPUT  =   128 # 256\n",
    "    IMG_MODEL_NAME = \"swinV2\" #\"efficientnet_v2\" # \n",
    "    IMG_FINED_TUNED_WEIGHT = f'{BASE_DIR}/swinV2.pth'\n",
    "    Lower_Quantile = 0.005\n",
    "    Upper_Quantile = 0.99 #0.985\n",
    "    # use XGBBOOST to find prominant features\n",
    "    EXTRA_COLOUMN = ['WORLDCLIM_BIO1_annual_mean_temperature',\n",
    "       'WORLDCLIM_BIO12_annual_precipitation',\n",
    "       'WORLDCLIM_BIO13.BIO14_delta_precipitation_of_wettest_and_dryest_month',\n",
    "       'WORLDCLIM_BIO15_precipitation_seasonality',\n",
    "       'WORLDCLIM_BIO4_temperature_seasonality',\n",
    "       'WORLDCLIM_BIO7_temperature_annual_range',\n",
    "       'SOIL_bdod_0.5cm_mean_0.01_deg',\n",
    "       'SOIL_bdod_100.200cm_mean_0.01_deg',\n",
    "       'SOIL_bdod_15.30cm_mean_0.01_deg',\n",
    "       'SOIL_bdod_30.60cm_mean_0.01_deg',\n",
    "       'SOIL_bdod_5.15cm_mean_0.01_deg',\n",
    "       'SOIL_bdod_60.100cm_mean_0.01_deg', 'SOIL_cec_0.5cm_mean_0.01_deg',\n",
    "       'SOIL_cec_100.200cm_mean_0.01_deg',\n",
    "       'SOIL_cec_15.30cm_mean_0.01_deg', 'SOIL_cec_30.60cm_mean_0.01_deg',\n",
    "       'SOIL_cec_5.15cm_mean_0.01_deg', 'SOIL_cec_60.100cm_mean_0.01_deg',\n",
    "       'SOIL_cfvo_0.5cm_mean_0.01_deg',\n",
    "       'SOIL_cfvo_100.200cm_mean_0.01_deg',\n",
    "       'SOIL_cfvo_15.30cm_mean_0.01_deg',\n",
    "       'SOIL_cfvo_30.60cm_mean_0.01_deg',\n",
    "       'SOIL_cfvo_5.15cm_mean_0.01_deg',\n",
    "       'SOIL_cfvo_60.100cm_mean_0.01_deg',\n",
    "       'SOIL_clay_0.5cm_mean_0.01_deg',\n",
    "       'SOIL_clay_100.200cm_mean_0.01_deg',\n",
    "       'SOIL_clay_15.30cm_mean_0.01_deg',\n",
    "       'SOIL_clay_30.60cm_mean_0.01_deg',\n",
    "       'SOIL_clay_5.15cm_mean_0.01_deg',\n",
    "       'SOIL_clay_60.100cm_mean_0.01_deg',\n",
    "       'SOIL_nitrogen_0.5cm_mean_0.01_deg',\n",
    "       'SOIL_nitrogen_100.200cm_mean_0.01_deg',\n",
    "       'SOIL_nitrogen_15.30cm_mean_0.01_deg',\n",
    "       'SOIL_nitrogen_30.60cm_mean_0.01_deg',\n",
    "       'SOIL_nitrogen_5.15cm_mean_0.01_deg',\n",
    "       'SOIL_nitrogen_60.100cm_mean_0.01_deg',\n",
    "       'SOIL_ocd_0.5cm_mean_0.01_deg', 'SOIL_ocd_100.200cm_mean_0.01_deg',\n",
    "       'SOIL_ocd_15.30cm_mean_0.01_deg', 'SOIL_ocd_30.60cm_mean_0.01_deg',\n",
    "       'SOIL_ocd_5.15cm_mean_0.01_deg', 'SOIL_ocd_60.100cm_mean_0.01_deg',\n",
    "       'SOIL_ocs_0.30cm_mean_0.01_deg', 'SOIL_phh2o_0.5cm_mean_0.01_deg',\n",
    "       'SOIL_phh2o_100.200cm_mean_0.01_deg',\n",
    "       'SOIL_phh2o_15.30cm_mean_0.01_deg',\n",
    "       'SOIL_phh2o_30.60cm_mean_0.01_deg',\n",
    "       'SOIL_phh2o_5.15cm_mean_0.01_deg',\n",
    "       'SOIL_phh2o_60.100cm_mean_0.01_deg',\n",
    "       'SOIL_sand_0.5cm_mean_0.01_deg',\n",
    "       'SOIL_sand_100.200cm_mean_0.01_deg',\n",
    "       'SOIL_sand_15.30cm_mean_0.01_deg',\n",
    "       'SOIL_sand_30.60cm_mean_0.01_deg',\n",
    "       'SOIL_sand_5.15cm_mean_0.01_deg',\n",
    "       'SOIL_sand_60.100cm_mean_0.01_deg',\n",
    "       'SOIL_silt_0.5cm_mean_0.01_deg',\n",
    "       'SOIL_silt_100.200cm_mean_0.01_deg',\n",
    "       'SOIL_silt_15.30cm_mean_0.01_deg',\n",
    "       'SOIL_silt_30.60cm_mean_0.01_deg',\n",
    "       'SOIL_silt_5.15cm_mean_0.01_deg',\n",
    "       'SOIL_silt_60.100cm_mean_0.01_deg', 'SOIL_soc_0.5cm_mean_0.01_deg',\n",
    "       'SOIL_soc_100.200cm_mean_0.01_deg',\n",
    "       'SOIL_soc_15.30cm_mean_0.01_deg', 'SOIL_soc_30.60cm_mean_0.01_deg',\n",
    "       'SOIL_soc_5.15cm_mean_0.01_deg', 'SOIL_soc_60.100cm_mean_0.01_deg',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_01_._month_m1',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_02_._month_m1',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_03_._month_m1',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_04_._month_m1',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_05_._month_m1',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_01_._month_m10',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_02_._month_m10',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_03_._month_m10',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_04_._month_m10',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_05_._month_m10',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_01_._month_m11',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_02_._month_m11',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_03_._month_m11',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_04_._month_m11',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_05_._month_m11',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_01_._month_m12',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_02_._month_m12',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_03_._month_m12',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_04_._month_m12',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_05_._month_m12',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_01_._month_m2',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_02_._month_m2',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_03_._month_m2',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_04_._month_m2',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_05_._month_m2',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_01_._month_m3',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_02_._month_m3',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_03_._month_m3',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_04_._month_m3',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_05_._month_m3',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_01_._month_m4',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_02_._month_m4',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_03_._month_m4',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_04_._month_m4',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_05_._month_m4',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_01_._month_m5',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_02_._month_m5',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_03_._month_m5',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_04_._month_m5',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_05_._month_m5',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_01_._month_m6',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_02_._month_m6',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_03_._month_m6',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_04_._month_m6',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_05_._month_m6',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_01_._month_m7',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_02_._month_m7',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_03_._month_m7',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_04_._month_m7',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_05_._month_m7',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_01_._month_m8',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_02_._month_m8',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_03_._month_m8',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_04_._month_m8',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_05_._month_m8',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_01_._month_m9',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_02_._month_m9',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_03_._month_m9',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_04_._month_m9',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_05_._month_m9',\n",
    "       'VOD_C_2002_2018_multiyear_mean_m01',\n",
    "       'VOD_C_2002_2018_multiyear_mean_m02',\n",
    "       'VOD_C_2002_2018_multiyear_mean_m03',\n",
    "       'VOD_C_2002_2018_multiyear_mean_m04',\n",
    "       'VOD_C_2002_2018_multiyear_mean_m05',\n",
    "       'VOD_C_2002_2018_multiyear_mean_m06',\n",
    "       'VOD_C_2002_2018_multiyear_mean_m07',\n",
    "       'VOD_C_2002_2018_multiyear_mean_m08',\n",
    "       'VOD_C_2002_2018_multiyear_mean_m09',\n",
    "       'VOD_C_2002_2018_multiyear_mean_m10',\n",
    "       'VOD_C_2002_2018_multiyear_mean_m11',\n",
    "       'VOD_C_2002_2018_multiyear_mean_m12',\n",
    "       'VOD_Ku_1987_2017_multiyear_mean_m01',\n",
    "       'VOD_Ku_1987_2017_multiyear_mean_m02',\n",
    "       'VOD_Ku_1987_2017_multiyear_mean_m03',\n",
    "       'VOD_Ku_1987_2017_multiyear_mean_m04',\n",
    "       'VOD_Ku_1987_2017_multiyear_mean_m05',\n",
    "       'VOD_Ku_1987_2017_multiyear_mean_m06',\n",
    "       'VOD_Ku_1987_2017_multiyear_mean_m07',\n",
    "       'VOD_Ku_1987_2017_multiyear_mean_m08',\n",
    "       'VOD_Ku_1987_2017_multiyear_mean_m09',\n",
    "       'VOD_Ku_1987_2017_multiyear_mean_m10',\n",
    "       'VOD_Ku_1987_2017_multiyear_mean_m11',\n",
    "       'VOD_Ku_1987_2017_multiyear_mean_m12',\n",
    "       'VOD_X_1997_2018_multiyear_mean_m01',\n",
    "       'VOD_X_1997_2018_multiyear_mean_m02',\n",
    "       'VOD_X_1997_2018_multiyear_mean_m03',\n",
    "       'VOD_X_1997_2018_multiyear_mean_m04',\n",
    "       'VOD_X_1997_2018_multiyear_mean_m05',\n",
    "       'VOD_X_1997_2018_multiyear_mean_m06',\n",
    "       'VOD_X_1997_2018_multiyear_mean_m07',\n",
    "       'VOD_X_1997_2018_multiyear_mean_m08',\n",
    "       'VOD_X_1997_2018_multiyear_mean_m09',\n",
    "       'VOD_X_1997_2018_multiyear_mean_m10',\n",
    "       'VOD_X_1997_2018_multiyear_mean_m11',\n",
    "       'VOD_X_1997_2018_multiyear_mean_m12'\n",
    "       ]\n",
    "    N_TARGETS  =len(TRAITS_NAME)  \n",
    "\n",
    "CONFIG = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JPEG Files Processing:\n",
      "JPEG Files Processing End\n"
     ]
    }
   ],
   "source": [
    "log_tf_col = CONFIG.TRAITS_NAME\n",
    "scale_feature_col =  CONFIG.EXTRA_COLOUMN + CONFIG.TRAITS_NAME\n",
    "log_transform = ColumnTransformer(transformers= [\n",
    "    ('log' , FunctionTransformer( np.log10  , inverse_func=sp.special.exp10, validate=False, check_inverse = True ,feature_names_out='one-to-one') , log_tf_col)\n",
    "        ] , verbose_feature_names_out=False ,remainder= 'passthrough'\n",
    "          )\n",
    "log_transform.set_output(transform='pandas')\n",
    "\n",
    "std_scale =  ColumnTransformer(transformers=[('scale',StandardScaler() ,scale_feature_col )  ],\n",
    "                               verbose_feature_names_out=False,\n",
    "                               remainder='passthrough'\n",
    "                               )\n",
    "std_scale.set_output(transform='pandas')\n",
    "scaling_pipeline = Pipeline(steps=[   \n",
    "                        (\"log\" , log_transform),\n",
    "                        (\"std_scale\" , std_scale )])\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "# preparing the tabular data that has been given \n",
    "BASE_DIR =CONFIG.BASE_DIR\n",
    "Train_DF = pd.read_csv(BASE_DIR  +  '/train.csv')  #.iloc[:-10000]\n",
    "Test_DF =  pd.read_csv(BASE_DIR  +  '/test.csv')\n",
    "Test_DF[log_tf_col] =1\n",
    "\n",
    "train_df , val_df = train_test_split(Train_DF,test_size=CONFIG.TRAIN_VAL_SPLIT_SIZE,shuffle=True)\n",
    "for column in CONFIG.TRAITS_NAME:\n",
    "    lower_quantile = train_df[column].quantile(CONFIG.Lower_Quantile)\n",
    "    upper_quantile = train_df[column].quantile(CONFIG.Upper_Quantile)\n",
    "    train_df = train_df[(train_df[column] >= lower_quantile) & (train_df[column] <= upper_quantile)]\n",
    "\n",
    "\n",
    "\n",
    "print('JPEG Files Processing:')\n",
    "train_df['file_path'] = train_df['id'].apply(lambda s: f'{BASE_DIR}/train_images/{s}.jpeg')\n",
    "train_df['jpeg_bytes'] = train_df['file_path'].apply(lambda fp: open(fp, 'rb').read())\n",
    "\n",
    "val_df['file_path'] = val_df['id'].apply(lambda s: f'{BASE_DIR}/train_images/{s}.jpeg')\n",
    "val_df['jpeg_bytes'] = val_df['file_path'].apply(lambda fp: open(fp, 'rb').read())\n",
    "\n",
    "Test_DF['file_path'] = Test_DF['id'].apply(lambda s: f'{BASE_DIR}/test_images/{s}.jpeg')\n",
    "Test_DF['jpeg_bytes'] = Test_DF['file_path'].apply(lambda fp: open(fp, 'rb').read())\n",
    "print('JPEG Files Processing End')    \n",
    "\n",
    "\n",
    "# train_tabular = train_df.drop(columns = ['id'] + CONFIG.TRAITS_NAME)\n",
    "# val_tabular =    val_df.drop(columns = ['id'] + CONFIG.TRAITS_NAME)\n",
    "# test_tabular = test.drop(columns = ['id'])\n",
    "\n",
    "# scaling of the training data !\n",
    "\n",
    "training_scaling_pipeline = Pipeline(steps=[   \n",
    "                        (\"log\" , log_transform),\n",
    "                        (\"std_scale\" , std_scale )])\n",
    "\n",
    "train_tabular_scaled =  training_scaling_pipeline.fit_transform(train_df)\n",
    "validation_tabular_scaled = training_scaling_pipeline.transform(val_df)\n",
    "\n",
    "test_tabular_scaled = training_scaling_pipeline.transform(Test_DF)\n",
    "Test_DF = Test_DF.drop(columns=log_tf_col)\n",
    "             \n",
    "                \n",
    "\n",
    "# some hyper parameter for lr scheduler\n",
    "CONFIG.N_TRAIN_SAMPLES = len(train_df)\n",
    "CONFIG.N_STEPS_PER_EPOCH = (CONFIG.N_TRAIN_SAMPLES // CONFIG.TRAIN_BATCH_SIZE)\n",
    "CONFIG.N_STEPS = CONFIG.N_STEPS_PER_EPOCH * CONFIG.NUM_EPOCHS + 1   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEAN = [0.485, 0.456, 0.406]\n",
    "STD = [0.229, 0.224, 0.225]\n",
    "TRAIN_TRANSFORMS = A.Compose([\n",
    "    A.RandomResizedCrop(size=(CONFIG.TARGET_IMAGE_SIZE,CONFIG.TARGET_IMAGE_SIZE), interpolation=cv2.INTER_CUBIC),  # Simulate different crops\n",
    "    #A.HorizontalFlip(p=0.2),  \n",
    "    A.GaussianBlur(blur_limit=(3, 7), p=0.5),  # Introduce slight blur\n",
    "    A.ToFloat(),\n",
    "    A.Normalize(mean=MEAN, std=STD, max_pixel_value=1),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "TEST_TRANSFORMS = A.Compose([\n",
    "    A.Resize(CONFIG.TARGET_IMAGE_SIZE,CONFIG.TARGET_IMAGE_SIZE),\n",
    "    #A.CenterCrop(CONFIG.TARGET_IMAGE_SIZE,CONFIG.TARGET_IMAGE_SIZE),\n",
    "    A.ToFloat(),\n",
    "    A.Normalize(mean=MEAN, std=STD, max_pixel_value=1),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# %%\n",
    "class create_dataset(Dataset):\n",
    "    def __init__(self, X_jpeg_bytes, X_tabular, y, transforms=None):\n",
    "        self.X_jpeg_bytes = X_jpeg_bytes\n",
    "        self.X_tabular = X_tabular\n",
    "        self.y = y\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X_sample = self.transforms(\n",
    "            image=imageio.imread(self.X_jpeg_bytes[index]),\n",
    "        )['image']\n",
    "        X_tabular_sample = self.X_tabular[index]\n",
    "        y_sample = self.y[index]\n",
    "\n",
    "        return X_sample, X_tabular_sample, y_sample\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = create_dataset(\n",
    "    X_jpeg_bytes = train_tabular_scaled['jpeg_bytes'].values,\n",
    "    X_tabular = train_tabular_scaled[CONFIG.EXTRA_COLOUMN].values.astype(np.float32),\n",
    "    y = train_tabular_scaled[CONFIG.TRAITS_NAME].values.astype(np.float32),\n",
    "    transforms= TRAIN_TRANSFORMS\n",
    ")\n",
    "\n",
    "validation_dataset = create_dataset(\n",
    "    X_jpeg_bytes = validation_tabular_scaled['jpeg_bytes'].values,\n",
    "    X_tabular = validation_tabular_scaled[CONFIG.EXTRA_COLOUMN].values.astype(np.float32),\n",
    "    y = validation_tabular_scaled[CONFIG.TRAITS_NAME].values.astype(np.float32),\n",
    "    transforms= TEST_TRANSFORMS\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CONFIG.TRAIN_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=  0 #psutil.cpu_count(),\n",
    ")\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "    validation_dataset,\n",
    "    batch_size=CONFIG.VAL_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers= 0 #psutil.cpu_count(),\n",
    ")\n",
    "\n",
    "test_dataset = create_dataset(\n",
    "    X_jpeg_bytes = test_tabular_scaled['jpeg_bytes'].values,\n",
    "    X_tabular = test_tabular_scaled[CONFIG.EXTRA_COLOUMN].values.astype(np.float32),\n",
    "    y = test_tabular_scaled[\"id\"].values,\n",
    "    transforms= TEST_TRANSFORMS\n",
    "    )\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=CONFIG.TEST_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers= 0 #psutil.cpu_count(),\n",
    ")\n",
    "\n",
    "\n",
    "# %%\n",
    "\n",
    "\n",
    "\n",
    "class BestModelSaveCallback:\n",
    "    def __init__(self, save_path):\n",
    "        self.save_path = save_path\n",
    "        self.best_accuracy = -1\n",
    "\n",
    "    def __call__(self, accuracy,model):\n",
    "        if accuracy > self.best_accuracy:\n",
    "            self.best_accuracy = accuracy\n",
    "            model.to(device = \"cpu\")\n",
    "            torch.save(model.state_dict(), self.save_path)\n",
    "            model.to(device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ImageBackbone_swinV2(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.out_features = 6\n",
    "#         self.backbone = timm.create_model('swinv2_large_window12to16_192to256.ms_in22k_ft_in1k', pretrained=True, num_classes=6) #remove classifier nn.Linear\n",
    "#         #self.backbone = backbone_.forward_head(backbone_, pre_logits=True)\n",
    "#         in_features = self.backbone.num_features\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.backbone(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "# # %%\n",
    "# def initialize_image_model( model_name   , tim_num_class=0.0, fine_tuned_weight = None,fixed_feature_extractor=True):\n",
    "#     model_ft  = None\n",
    "#     if model_name == \"swinV2\":\n",
    "#         model = ImageBackbone_swinV2()\n",
    "#         return model\n",
    "\n",
    "# class CustomModel(nn.Module):\n",
    "#     def __init__(self,):\n",
    "#         super().__init__()\n",
    "#         self.img_backbone = initialize_image_model(\"swinV2\")       \n",
    "#     def forward(self,image,x):\n",
    "#         output = self.img_backbone(image) # bach * (hight*col)\n",
    "#         return output\n",
    "# model = CustomModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomModel(\n",
       "  (img_backbone): ImageBackbone_swinV2(\n",
       "    (backbone): SwinTransformerV2(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))\n",
       "        (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (layers): Sequential(\n",
       "        (0): SwinTransformerV2Stage(\n",
       "          (downsample): Identity()\n",
       "          (blocks): ModuleList(\n",
       "            (0): SwinTransformerV2Block(\n",
       "              (attn): WindowAttention(\n",
       "                (cpb_mlp): Sequential(\n",
       "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                  (1): ReLU(inplace=True)\n",
       "                  (2): Linear(in_features=512, out_features=6, bias=False)\n",
       "                )\n",
       "                (qkv): Linear(in_features=192, out_features=576, bias=False)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path1): Identity()\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "                (act): GELU()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path2): Identity()\n",
       "            )\n",
       "            (1): SwinTransformerV2Block(\n",
       "              (attn): WindowAttention(\n",
       "                (cpb_mlp): Sequential(\n",
       "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                  (1): ReLU(inplace=True)\n",
       "                  (2): Linear(in_features=512, out_features=6, bias=False)\n",
       "                )\n",
       "                (qkv): Linear(in_features=192, out_features=576, bias=False)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path1): DropPath(drop_prob=0.004)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
       "                (act): GELU()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path2): DropPath(drop_prob=0.004)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): SwinTransformerV2Stage(\n",
       "          (downsample): PatchMerging(\n",
       "            (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
       "            (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (blocks): ModuleList(\n",
       "            (0): SwinTransformerV2Block(\n",
       "              (attn): WindowAttention(\n",
       "                (cpb_mlp): Sequential(\n",
       "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                  (1): ReLU(inplace=True)\n",
       "                  (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "                )\n",
       "                (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path1): DropPath(drop_prob=0.009)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path2): DropPath(drop_prob=0.009)\n",
       "            )\n",
       "            (1): SwinTransformerV2Block(\n",
       "              (attn): WindowAttention(\n",
       "                (cpb_mlp): Sequential(\n",
       "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                  (1): ReLU(inplace=True)\n",
       "                  (2): Linear(in_features=512, out_features=12, bias=False)\n",
       "                )\n",
       "                (qkv): Linear(in_features=384, out_features=1152, bias=False)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path1): DropPath(drop_prob=0.013)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (act): GELU()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path2): DropPath(drop_prob=0.013)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): SwinTransformerV2Stage(\n",
       "          (downsample): PatchMerging(\n",
       "            (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
       "            (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (blocks): ModuleList(\n",
       "            (0): SwinTransformerV2Block(\n",
       "              (attn): WindowAttention(\n",
       "                (cpb_mlp): Sequential(\n",
       "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                  (1): ReLU(inplace=True)\n",
       "                  (2): Linear(in_features=512, out_features=24, bias=False)\n",
       "                )\n",
       "                (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path1): DropPath(drop_prob=0.017)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (act): GELU()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path2): DropPath(drop_prob=0.017)\n",
       "            )\n",
       "            (1): SwinTransformerV2Block(\n",
       "              (attn): WindowAttention(\n",
       "                (cpb_mlp): Sequential(\n",
       "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                  (1): ReLU(inplace=True)\n",
       "                  (2): Linear(in_features=512, out_features=24, bias=False)\n",
       "                )\n",
       "                (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path1): DropPath(drop_prob=0.022)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (act): GELU()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path2): DropPath(drop_prob=0.022)\n",
       "            )\n",
       "            (2): SwinTransformerV2Block(\n",
       "              (attn): WindowAttention(\n",
       "                (cpb_mlp): Sequential(\n",
       "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                  (1): ReLU(inplace=True)\n",
       "                  (2): Linear(in_features=512, out_features=24, bias=False)\n",
       "                )\n",
       "                (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path1): DropPath(drop_prob=0.026)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (act): GELU()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path2): DropPath(drop_prob=0.026)\n",
       "            )\n",
       "            (3): SwinTransformerV2Block(\n",
       "              (attn): WindowAttention(\n",
       "                (cpb_mlp): Sequential(\n",
       "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                  (1): ReLU(inplace=True)\n",
       "                  (2): Linear(in_features=512, out_features=24, bias=False)\n",
       "                )\n",
       "                (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path1): DropPath(drop_prob=0.030)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (act): GELU()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path2): DropPath(drop_prob=0.030)\n",
       "            )\n",
       "            (4): SwinTransformerV2Block(\n",
       "              (attn): WindowAttention(\n",
       "                (cpb_mlp): Sequential(\n",
       "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                  (1): ReLU(inplace=True)\n",
       "                  (2): Linear(in_features=512, out_features=24, bias=False)\n",
       "                )\n",
       "                (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path1): DropPath(drop_prob=0.035)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (act): GELU()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path2): DropPath(drop_prob=0.035)\n",
       "            )\n",
       "            (5): SwinTransformerV2Block(\n",
       "              (attn): WindowAttention(\n",
       "                (cpb_mlp): Sequential(\n",
       "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                  (1): ReLU(inplace=True)\n",
       "                  (2): Linear(in_features=512, out_features=24, bias=False)\n",
       "                )\n",
       "                (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path1): DropPath(drop_prob=0.039)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (act): GELU()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path2): DropPath(drop_prob=0.039)\n",
       "            )\n",
       "            (6): SwinTransformerV2Block(\n",
       "              (attn): WindowAttention(\n",
       "                (cpb_mlp): Sequential(\n",
       "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                  (1): ReLU(inplace=True)\n",
       "                  (2): Linear(in_features=512, out_features=24, bias=False)\n",
       "                )\n",
       "                (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path1): DropPath(drop_prob=0.043)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (act): GELU()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path2): DropPath(drop_prob=0.043)\n",
       "            )\n",
       "            (7): SwinTransformerV2Block(\n",
       "              (attn): WindowAttention(\n",
       "                (cpb_mlp): Sequential(\n",
       "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                  (1): ReLU(inplace=True)\n",
       "                  (2): Linear(in_features=512, out_features=24, bias=False)\n",
       "                )\n",
       "                (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path1): DropPath(drop_prob=0.048)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (act): GELU()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path2): DropPath(drop_prob=0.048)\n",
       "            )\n",
       "            (8): SwinTransformerV2Block(\n",
       "              (attn): WindowAttention(\n",
       "                (cpb_mlp): Sequential(\n",
       "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                  (1): ReLU(inplace=True)\n",
       "                  (2): Linear(in_features=512, out_features=24, bias=False)\n",
       "                )\n",
       "                (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path1): DropPath(drop_prob=0.052)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (act): GELU()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path2): DropPath(drop_prob=0.052)\n",
       "            )\n",
       "            (9): SwinTransformerV2Block(\n",
       "              (attn): WindowAttention(\n",
       "                (cpb_mlp): Sequential(\n",
       "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                  (1): ReLU(inplace=True)\n",
       "                  (2): Linear(in_features=512, out_features=24, bias=False)\n",
       "                )\n",
       "                (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path1): DropPath(drop_prob=0.057)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (act): GELU()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path2): DropPath(drop_prob=0.057)\n",
       "            )\n",
       "            (10): SwinTransformerV2Block(\n",
       "              (attn): WindowAttention(\n",
       "                (cpb_mlp): Sequential(\n",
       "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                  (1): ReLU(inplace=True)\n",
       "                  (2): Linear(in_features=512, out_features=24, bias=False)\n",
       "                )\n",
       "                (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path1): DropPath(drop_prob=0.061)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (act): GELU()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path2): DropPath(drop_prob=0.061)\n",
       "            )\n",
       "            (11): SwinTransformerV2Block(\n",
       "              (attn): WindowAttention(\n",
       "                (cpb_mlp): Sequential(\n",
       "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                  (1): ReLU(inplace=True)\n",
       "                  (2): Linear(in_features=512, out_features=24, bias=False)\n",
       "                )\n",
       "                (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path1): DropPath(drop_prob=0.065)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (act): GELU()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path2): DropPath(drop_prob=0.065)\n",
       "            )\n",
       "            (12): SwinTransformerV2Block(\n",
       "              (attn): WindowAttention(\n",
       "                (cpb_mlp): Sequential(\n",
       "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                  (1): ReLU(inplace=True)\n",
       "                  (2): Linear(in_features=512, out_features=24, bias=False)\n",
       "                )\n",
       "                (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path1): DropPath(drop_prob=0.070)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (act): GELU()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path2): DropPath(drop_prob=0.070)\n",
       "            )\n",
       "            (13): SwinTransformerV2Block(\n",
       "              (attn): WindowAttention(\n",
       "                (cpb_mlp): Sequential(\n",
       "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                  (1): ReLU(inplace=True)\n",
       "                  (2): Linear(in_features=512, out_features=24, bias=False)\n",
       "                )\n",
       "                (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path1): DropPath(drop_prob=0.074)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (act): GELU()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path2): DropPath(drop_prob=0.074)\n",
       "            )\n",
       "            (14): SwinTransformerV2Block(\n",
       "              (attn): WindowAttention(\n",
       "                (cpb_mlp): Sequential(\n",
       "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                  (1): ReLU(inplace=True)\n",
       "                  (2): Linear(in_features=512, out_features=24, bias=False)\n",
       "                )\n",
       "                (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path1): DropPath(drop_prob=0.078)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (act): GELU()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path2): DropPath(drop_prob=0.078)\n",
       "            )\n",
       "            (15): SwinTransformerV2Block(\n",
       "              (attn): WindowAttention(\n",
       "                (cpb_mlp): Sequential(\n",
       "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                  (1): ReLU(inplace=True)\n",
       "                  (2): Linear(in_features=512, out_features=24, bias=False)\n",
       "                )\n",
       "                (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path1): DropPath(drop_prob=0.083)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (act): GELU()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path2): DropPath(drop_prob=0.083)\n",
       "            )\n",
       "            (16): SwinTransformerV2Block(\n",
       "              (attn): WindowAttention(\n",
       "                (cpb_mlp): Sequential(\n",
       "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                  (1): ReLU(inplace=True)\n",
       "                  (2): Linear(in_features=512, out_features=24, bias=False)\n",
       "                )\n",
       "                (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path1): DropPath(drop_prob=0.087)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (act): GELU()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path2): DropPath(drop_prob=0.087)\n",
       "            )\n",
       "            (17): SwinTransformerV2Block(\n",
       "              (attn): WindowAttention(\n",
       "                (cpb_mlp): Sequential(\n",
       "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                  (1): ReLU(inplace=True)\n",
       "                  (2): Linear(in_features=512, out_features=24, bias=False)\n",
       "                )\n",
       "                (qkv): Linear(in_features=768, out_features=2304, bias=False)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path1): DropPath(drop_prob=0.091)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (act): GELU()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path2): DropPath(drop_prob=0.091)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): SwinTransformerV2Stage(\n",
       "          (downsample): PatchMerging(\n",
       "            (reduction): Linear(in_features=3072, out_features=1536, bias=False)\n",
       "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (blocks): ModuleList(\n",
       "            (0): SwinTransformerV2Block(\n",
       "              (attn): WindowAttention(\n",
       "                (cpb_mlp): Sequential(\n",
       "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                  (1): ReLU(inplace=True)\n",
       "                  (2): Linear(in_features=512, out_features=48, bias=False)\n",
       "                )\n",
       "                (qkv): Linear(in_features=1536, out_features=4608, bias=False)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path1): DropPath(drop_prob=0.096)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
       "                (act): GELU()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path2): DropPath(drop_prob=0.096)\n",
       "            )\n",
       "            (1): SwinTransformerV2Block(\n",
       "              (attn): WindowAttention(\n",
       "                (cpb_mlp): Sequential(\n",
       "                  (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                  (1): ReLU(inplace=True)\n",
       "                  (2): Linear(in_features=512, out_features=48, bias=False)\n",
       "                )\n",
       "                (qkv): Linear(in_features=1536, out_features=4608, bias=False)\n",
       "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "                (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "                (softmax): Softmax(dim=-1)\n",
       "              )\n",
       "              (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path1): DropPath(drop_prob=0.100)\n",
       "              (mlp): Mlp(\n",
       "                (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
       "                (act): GELU()\n",
       "                (drop1): Dropout(p=0.0, inplace=False)\n",
       "                (norm): Identity()\n",
       "                (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n",
       "                (drop2): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path2): DropPath(drop_prob=0.100)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "      (head): Identity()\n",
       "    )\n",
       "    (head): Sequential(\n",
       "      (0): AdaptiveAvgPool2d(output_size=1)\n",
       "      (1): Flatten(start_dim=1, end_dim=-1)\n",
       "      (2): Linear(in_features=1536, out_features=1024, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (extra_features_model): TabularBackbone(\n",
       "    (fc): Sequential(\n",
       "      (0): Linear(in_features=163, out_features=128, bias=True)\n",
       "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): Dropout(p=0.1, inplace=False)\n",
       "      (4): Linear(in_features=128, out_features=64, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): Linear(in_features=1088, out_features=256, bias=True)\n",
       "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): GELU(approximate='none')\n",
       "    (3): Dropout(p=0.1, inplace=False)\n",
       "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): GELU(approximate='none')\n",
       "    (7): Linear(in_features=128, out_features=6, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ImageBackbone_swinV2(nn.Module):\n",
    "    def __init__(self, backbone_name, weight_path, out_features, fixed_feature_extractor=False):\n",
    "        super().__init__()\n",
    "        self.out_features = out_features\n",
    "        self.backbone =timm.create_model('swinv2_large_window12to16_192to256.ms_in22k_ft_in1k', pretrained=False, num_classes=CONFIG.N_TARGETS) # timm.create_model('swin_large_patch4_window12_384.ms_in22k_ft_in1k', pretrained=True, num_classes=CONFIG.N_TARGETS)\n",
    "        swin_fine_tuned_weight = torch.load(weight_path)\n",
    "        swin_fine_tuned_weight  = {key.replace(\"img_backbone.backbone.\", \"\"): value for key, value in swin_fine_tuned_weight.items()}\n",
    "        self.backbone.load_state_dict(swin_fine_tuned_weight)\n",
    "        in_features = self.backbone.num_features\n",
    "\n",
    "        self.backbone.head = nn.Identity()\n",
    "        self.head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features, out_features),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        return self.head(x)\n",
    "\n",
    "class TabularBackbone(nn.Module):\n",
    "    def __init__(self, n_features, out_features):\n",
    "        super().__init__()\n",
    "        self.out_features = out_features\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(n_features, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(128, out_features),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)    \n",
    "\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# # Combine the image feature extraction model with tabular feature extraction model \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#backbone_name, weight_path, out_features, fixed_feature_extractor=False\n",
    "# %%\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self,model_name , fine_tuned_img_weights , img_out_features , fixed_feature_extractor , traits_target_feature):\n",
    "        super().__init__()\n",
    "        self.img_backbone =  ImageBackbone_swinV2(backbone_name = model_name,weight_path = fine_tuned_img_weights ,out_features =  img_out_features ,fixed_feature_extractor=fixed_feature_extractor)\n",
    "        self.extra_features_model = TabularBackbone(n_features  = len(CONFIG.EXTRA_COLOUMN) , out_features=64)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.extra_features_model.out_features + self.img_backbone.out_features, 256),  #1024\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, 128), #256\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.GELU(),\n",
    "            #nn.Dropout(0.1),\n",
    "            nn.Linear(128, traits_target_feature),\n",
    "        )        \n",
    "    def forward(self,image,x):\n",
    "        output_image = self.img_backbone(image) \n",
    "        z = self.extra_features_model(x) \n",
    "        inputs  = torch.cat((output_image,z), 1 )\n",
    "        output = self.fc(inputs)\n",
    "        return output\n",
    "\n",
    "model = CustomModel(model_name=CONFIG.IMG_MODEL_NAME , fine_tuned_img_weights= CONFIG.IMG_FINED_TUNED_WEIGHT,img_out_features = CONFIG.IMG_OUT_FEATURES, fixed_feature_extractor=True,traits_target_feature = CONFIG.N_TARGETS )\n",
    "model.to(DEVICE)      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "swinV2 = torch.load('/home/prajwal/cs680/cs680_kaggle/data/swinV2_large_self_fine_tuned_V2.pth')\n",
    "# model.load_state_dict(swinV2)\n",
    "swinV2.to(DEVICE)\n",
    "swinV2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_269709/2138857394.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  backbone.load_state_dict(torch.load('/home/prajwal/cs680/cs680_kaggle/data/model_08.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# backbone =timm.create_model('swinv2_large_window12to16_192to256.ms_in22k_ft_in1k', pretrained=False, num_classes=CONFIG.N_TARGETS) # timm.create_model('swin_large_patch4_window12_384.ms_in22k_ft_in1k', pretrained=True, num_classes=CONFIG.N_TARGETS)\n",
    "# backbone.load_state_dict(torch.load('/home/prajwal/cs680/cs680_kaggle/data/model_08.pth'))\n",
    "# backbone.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/tmp/ipykernel_276688/3430024869.py:19: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  submission_df = pd.concat([submission_df, pred_final.assign(id=test_id.cpu().detach().numpy())], ignore_index=True)\n",
      "50it [01:34,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submit!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "submission_df = pd.DataFrame(columns=CONFIG.TRAITS_NAME)\n",
    "for index , batch in tqdm.tqdm(enumerate(iter(test_dataloader))):\n",
    "    X_img_test = batch[0] \n",
    "    X_features  = batch[1]\n",
    "    test_id  = batch[2]\n",
    "    #print(batch) \n",
    "    with torch.no_grad():\n",
    "        #print(X_img_test.shape, X_features.shape)\n",
    "        y_pred = swinV2(X_img_test.to(DEVICE),X_features.to(DEVICE)).detach().cpu().numpy()  #,X_features.to(DEVICE)\n",
    "    \n",
    "        pred_pd = pd.DataFrame(columns=CONFIG.EXTRA_COLOUMN + CONFIG.TRAITS_NAME)\n",
    "        pred_pd[CONFIG.EXTRA_COLOUMN] =-1\n",
    "        pred_pd[CONFIG.TRAITS_NAME] = y_pred \n",
    "\n",
    "        temp1 =   scaling_pipeline['std_scale']['scale'].inverse_transform(pred_pd)\n",
    "        temp2=    pd.DataFrame(temp1, columns=CONFIG.EXTRA_COLOUMN + CONFIG.TRAITS_NAME)\n",
    "        pred_final =   scaling_pipeline['log']['log'].inverse_transform(temp2[CONFIG.TRAITS_NAME])\n",
    "        #pred_final[\"id\"] = test_id.cpu().detach().numpy()\n",
    "        submission_df = pd.concat([submission_df, pred_final.assign(id=test_id.cpu().detach().numpy())], ignore_index=True)\n",
    "# submission_df.to_csv('submission_self_tuning.csv', index=False)\n",
    "submission_df[[\"id\"]  + CONFIG.TRAITS_NAME ].to_csv('submission_tuned_with_weight_swinV2.csv', index=False)\n",
    "print(\"Submit!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
