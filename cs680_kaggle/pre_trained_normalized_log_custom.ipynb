{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prajwal/anaconda3/envs/pytorch_gpu/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.12 (you have 1.4.10). Upgrade using: pip install --upgrade albumentations\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import cv2 \n",
    "import IPython\n",
    "from glob import glob\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tqdm\n",
    "#import seaborn as sns\n",
    "import albumentations as A\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "from torcheval.metrics import R2Score\n",
    "import wandb\n",
    "import torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x76e4de34bdb0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "BASE_DIR = os.path.join(os.getcwd() , 'data')\n",
    "train_df = pd.read_csv(BASE_DIR  +  '/train.csv')\n",
    "TRAIN_VAL_SPLIT_SIZE = 0.2\n",
    "TRAIN_BATCH_SIZE = 128\n",
    "VAL_BATCH_SIZE = 128\n",
    "TEST_BATCH_SIZE  = 128\n",
    "LEARNING_RATE = 1e-2\n",
    "EPOCHS = 26\n",
    "TIM_NUM_CLASS = 32\n",
    "Normalize_transform_type = \"log_transform\"\n",
    "RANDOM_NUMBER = 42\n",
    "torch.manual_seed(RANDOM_NUMBER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wandb.login()\n",
    "\n",
    "# wandb.init(project=\"cs680v2_normalized\",\n",
    "#     config={\n",
    "#        \"learning_rate\": LEARNING_RATE,\n",
    "#         \"epochs\": EPOCHS,\n",
    "#         \"batch_size\" : TRAIN_BATCH_SIZE,\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DATA_DIRECTORY = os.path.join(os.getcwd(),\"data\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXTRA_COLOUMN = ['WORLDCLIM_BIO1_annual_mean_temperature',\n",
    "       'WORLDCLIM_BIO12_annual_precipitation',\n",
    "       'WORLDCLIM_BIO13.BIO14_delta_precipitation_of_wettest_and_dryest_month',\n",
    "       'WORLDCLIM_BIO15_precipitation_seasonality',\n",
    "       'WORLDCLIM_BIO4_temperature_seasonality',\n",
    "       'WORLDCLIM_BIO7_temperature_annual_range',\n",
    "       'SOIL_bdod_0.5cm_mean_0.01_deg',\n",
    "       'SOIL_bdod_100.200cm_mean_0.01_deg',\n",
    "       'SOIL_bdod_15.30cm_mean_0.01_deg',\n",
    "       'SOIL_bdod_30.60cm_mean_0.01_deg',\n",
    "       'SOIL_bdod_5.15cm_mean_0.01_deg',\n",
    "       'SOIL_bdod_60.100cm_mean_0.01_deg', 'SOIL_cec_0.5cm_mean_0.01_deg',\n",
    "       'SOIL_cec_100.200cm_mean_0.01_deg',\n",
    "       'SOIL_cec_15.30cm_mean_0.01_deg', 'SOIL_cec_30.60cm_mean_0.01_deg',\n",
    "       'SOIL_cec_5.15cm_mean_0.01_deg', 'SOIL_cec_60.100cm_mean_0.01_deg',\n",
    "       'SOIL_cfvo_0.5cm_mean_0.01_deg',\n",
    "       'SOIL_cfvo_100.200cm_mean_0.01_deg',\n",
    "       'SOIL_cfvo_15.30cm_mean_0.01_deg',\n",
    "       'SOIL_cfvo_30.60cm_mean_0.01_deg',\n",
    "       'SOIL_cfvo_5.15cm_mean_0.01_deg',\n",
    "       'SOIL_cfvo_60.100cm_mean_0.01_deg',\n",
    "       'SOIL_clay_0.5cm_mean_0.01_deg',\n",
    "       'SOIL_clay_100.200cm_mean_0.01_deg',\n",
    "       'SOIL_clay_15.30cm_mean_0.01_deg',\n",
    "       'SOIL_clay_30.60cm_mean_0.01_deg',\n",
    "       'SOIL_clay_5.15cm_mean_0.01_deg',\n",
    "       'SOIL_clay_60.100cm_mean_0.01_deg',\n",
    "       'SOIL_nitrogen_0.5cm_mean_0.01_deg',\n",
    "       'SOIL_nitrogen_100.200cm_mean_0.01_deg',\n",
    "       'SOIL_nitrogen_15.30cm_mean_0.01_deg',\n",
    "       'SOIL_nitrogen_30.60cm_mean_0.01_deg',\n",
    "       'SOIL_nitrogen_5.15cm_mean_0.01_deg',\n",
    "       'SOIL_nitrogen_60.100cm_mean_0.01_deg',\n",
    "       'SOIL_ocd_0.5cm_mean_0.01_deg', 'SOIL_ocd_100.200cm_mean_0.01_deg',\n",
    "       'SOIL_ocd_15.30cm_mean_0.01_deg', 'SOIL_ocd_30.60cm_mean_0.01_deg',\n",
    "       'SOIL_ocd_5.15cm_mean_0.01_deg', 'SOIL_ocd_60.100cm_mean_0.01_deg',\n",
    "       'SOIL_ocs_0.30cm_mean_0.01_deg', 'SOIL_phh2o_0.5cm_mean_0.01_deg',\n",
    "       'SOIL_phh2o_100.200cm_mean_0.01_deg',\n",
    "       'SOIL_phh2o_15.30cm_mean_0.01_deg',\n",
    "       'SOIL_phh2o_30.60cm_mean_0.01_deg',\n",
    "       'SOIL_phh2o_5.15cm_mean_0.01_deg',\n",
    "       'SOIL_phh2o_60.100cm_mean_0.01_deg',\n",
    "       'SOIL_sand_0.5cm_mean_0.01_deg',\n",
    "       'SOIL_sand_100.200cm_mean_0.01_deg',\n",
    "       'SOIL_sand_15.30cm_mean_0.01_deg',\n",
    "       'SOIL_sand_30.60cm_mean_0.01_deg',\n",
    "       'SOIL_sand_5.15cm_mean_0.01_deg',\n",
    "       'SOIL_sand_60.100cm_mean_0.01_deg',\n",
    "       'SOIL_silt_0.5cm_mean_0.01_deg',\n",
    "       'SOIL_silt_100.200cm_mean_0.01_deg',\n",
    "       'SOIL_silt_15.30cm_mean_0.01_deg',\n",
    "       'SOIL_silt_30.60cm_mean_0.01_deg',\n",
    "       'SOIL_silt_5.15cm_mean_0.01_deg',\n",
    "       'SOIL_silt_60.100cm_mean_0.01_deg', 'SOIL_soc_0.5cm_mean_0.01_deg',\n",
    "       'SOIL_soc_100.200cm_mean_0.01_deg',\n",
    "       'SOIL_soc_15.30cm_mean_0.01_deg', 'SOIL_soc_30.60cm_mean_0.01_deg',\n",
    "       'SOIL_soc_5.15cm_mean_0.01_deg', 'SOIL_soc_60.100cm_mean_0.01_deg',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_01_._month_m1',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_02_._month_m1',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_03_._month_m1',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_04_._month_m1',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_05_._month_m1',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_01_._month_m10',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_02_._month_m10',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_03_._month_m10',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_04_._month_m10',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_05_._month_m10',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_01_._month_m11',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_02_._month_m11',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_03_._month_m11',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_04_._month_m11',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_05_._month_m11',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_01_._month_m12',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_02_._month_m12',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_03_._month_m12',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_04_._month_m12',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_05_._month_m12',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_01_._month_m2',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_02_._month_m2',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_03_._month_m2',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_04_._month_m2',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_05_._month_m2',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_01_._month_m3',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_02_._month_m3',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_03_._month_m3',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_04_._month_m3',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_05_._month_m3',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_01_._month_m4',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_02_._month_m4',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_03_._month_m4',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_04_._month_m4',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_05_._month_m4',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_01_._month_m5',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_02_._month_m5',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_03_._month_m5',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_04_._month_m5',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_05_._month_m5',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_01_._month_m6',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_02_._month_m6',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_03_._month_m6',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_04_._month_m6',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_05_._month_m6',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_01_._month_m7',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_02_._month_m7',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_03_._month_m7',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_04_._month_m7',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_05_._month_m7',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_01_._month_m8',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_02_._month_m8',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_03_._month_m8',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_04_._month_m8',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_05_._month_m8',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_01_._month_m9',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_02_._month_m9',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_03_._month_m9',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_04_._month_m9',\n",
    "       'MODIS_2000.2020_monthly_mean_surface_reflectance_band_05_._month_m9',\n",
    "       'VOD_C_2002_2018_multiyear_mean_m01',\n",
    "       'VOD_C_2002_2018_multiyear_mean_m02',\n",
    "       'VOD_C_2002_2018_multiyear_mean_m03',\n",
    "       'VOD_C_2002_2018_multiyear_mean_m04',\n",
    "       'VOD_C_2002_2018_multiyear_mean_m05',\n",
    "       'VOD_C_2002_2018_multiyear_mean_m06',\n",
    "       'VOD_C_2002_2018_multiyear_mean_m07',\n",
    "       'VOD_C_2002_2018_multiyear_mean_m08',\n",
    "       'VOD_C_2002_2018_multiyear_mean_m09',\n",
    "       'VOD_C_2002_2018_multiyear_mean_m10',\n",
    "       'VOD_C_2002_2018_multiyear_mean_m11',\n",
    "       'VOD_C_2002_2018_multiyear_mean_m12',\n",
    "       'VOD_Ku_1987_2017_multiyear_mean_m01',\n",
    "       'VOD_Ku_1987_2017_multiyear_mean_m02',\n",
    "       'VOD_Ku_1987_2017_multiyear_mean_m03',\n",
    "       'VOD_Ku_1987_2017_multiyear_mean_m04',\n",
    "       'VOD_Ku_1987_2017_multiyear_mean_m05',\n",
    "       'VOD_Ku_1987_2017_multiyear_mean_m06',\n",
    "       'VOD_Ku_1987_2017_multiyear_mean_m07',\n",
    "       'VOD_Ku_1987_2017_multiyear_mean_m08',\n",
    "       'VOD_Ku_1987_2017_multiyear_mean_m09',\n",
    "       'VOD_Ku_1987_2017_multiyear_mean_m10',\n",
    "       'VOD_Ku_1987_2017_multiyear_mean_m11',\n",
    "       'VOD_Ku_1987_2017_multiyear_mean_m12',\n",
    "       'VOD_X_1997_2018_multiyear_mean_m01',\n",
    "       'VOD_X_1997_2018_multiyear_mean_m02',\n",
    "       'VOD_X_1997_2018_multiyear_mean_m03',\n",
    "       'VOD_X_1997_2018_multiyear_mean_m04',\n",
    "       'VOD_X_1997_2018_multiyear_mean_m05',\n",
    "       'VOD_X_1997_2018_multiyear_mean_m06',\n",
    "       'VOD_X_1997_2018_multiyear_mean_m07',\n",
    "       'VOD_X_1997_2018_multiyear_mean_m08',\n",
    "       'VOD_X_1997_2018_multiyear_mean_m09',\n",
    "       'VOD_X_1997_2018_multiyear_mean_m10',\n",
    "       'VOD_X_1997_2018_multiyear_mean_m11',\n",
    "       'VOD_X_1997_2018_multiyear_mean_m12']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "SCALAR = StandardScaler()\n",
    "\n",
    "def create_val_train_dataset(BASE_DIR,train_split_size,scalar:StandardScaler):\n",
    "    df = pd.read_csv(BASE_DIR  +  '/train.csv')\n",
    "    df[\"image_path\"] = BASE_DIR + \"/train_images/\" + df[\"id\"].astype(str) + \".jpeg\"   # add images in the training dataframe\n",
    "    \n",
    "    # standardize the dataset\n",
    "    scalar.fit(df[EXTRA_COLOUMN])\n",
    "    df[EXTRA_COLOUMN] = scalar.transform(df[EXTRA_COLOUMN])\n",
    "    train_df , val_df = train_test_split(df,test_size=train_split_size,shuffle=True)\n",
    "    train_df.reset_index(drop=True , inplace  = True)\n",
    "    val_df.reset_index(drop = True , inplace = True)\n",
    "    return  train_df , val_df\n",
    "def create_test_dataset(BASE_DIR,scalar):\n",
    "    df = pd.read_csv(BASE_DIR  +  '/test.csv')\n",
    "    df[\"image_path\"] = BASE_DIR + \"/test_images/\" + df[\"id\"].astype(str) + \".jpeg\"   # add images in the training dataframe\n",
    "    df[EXTRA_COLOUMN] = scalar.transform(df[EXTRA_COLOUMN])\n",
    "    return df\n",
    "\n",
    "train_df , val_df = create_val_train_dataset(BASE_DIR,TRAIN_VAL_SPLIT_SIZE,scalar = SCALAR)\n",
    "test_df = create_test_dataset(BASE_DIR,scalar=SCALAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalizeOutput():\n",
    "    def __init__(self,type_transform ):\n",
    "        self.transform = type_transform\n",
    "        self.mean = 0\n",
    "        self.std = 0\n",
    "        self.mean_tensor = 0\n",
    "        self.std_tensor = 0\n",
    "        \n",
    "        pass\n",
    "    def normalize(self,df):\n",
    "        if self.transform == \"log_transform\":\n",
    "            mean = np.log10(df).mean()\n",
    "            std = np.log10(df).std()\n",
    "            self.mean = mean\n",
    "            self.std = std\n",
    "            self.mean_tensor = torch.Tensor(self.mean.values).to(DEVICE)\n",
    "            self.std_tensor = torch.Tensor(self.std.values).to(DEVICE)\n",
    "            return (np.log10(df) - self.mean)/self.std\n",
    "    def denormalize(self,df):\n",
    "        if self.mean is None or self.std is None :\n",
    "            raise Exception(\"mean and/std is not defined \")\n",
    "        if self.normalize == \"log_transform\":\n",
    "            df_denormalize =10**((df*self.std) + self.mean )\n",
    "            return df_denormalize\n",
    "    def denormalize_tensor(self,batch) :\n",
    "        if self.mean_tensor is None or self.std_tensor is None :\n",
    "            raise Exception(\"mean and/std is not defined \")\n",
    "        if self.transform == \"log_transform\":\n",
    "            df_denormalize =10**((batch*self.std_tensor) + self.mean_tensor )\n",
    "            return df_denormalize\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img = plt.imread(val_df[\"image_path\"].iloc[0])\n",
    "# plt.imshow(img)\n",
    "# img.shape\n",
    "#train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to avoid overfitting : 1) try diffferent transformations 2) batch norm 3)  dropout  4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outside pass\n",
      "outside pass\n"
     ]
    }
   ],
   "source": [
    "class data_loader(Dataset ):\n",
    "    def __init__(self,df , is_val = False,normalizedOutput=None, extra_params = False ):\n",
    "        global EXTRA_COLOUMN\n",
    "        self.df = df.copy()\n",
    "        if normalizedOutput == None:\n",
    "            pass\n",
    "        else :\n",
    "            if not is_val :\n",
    "                self.df[['X4_mean', 'X11_mean', 'X18_mean', 'X26_mean', 'X50_mean', 'X3112_mean' ]] = normalizedOutput.normalize(self.df[['X4_mean', 'X11_mean', 'X18_mean', 'X26_mean', 'X50_mean', 'X3112_mean' ]])\n",
    "        print(\"outside pass\")\n",
    "        self.transform =torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                                 #torchvision.transforms.Resize((384,384)),\n",
    "                                  torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]),\n",
    "                                    torchvision.transforms.RandomVerticalFlip(p=0.5),\n",
    "                                    torchvision.transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                    torchvision.transforms.RandomChoice([\n",
    "                                        torchvision.transforms.Lambda(lambda x : x + np.sqrt(0.1)*torch.randn_like(x)),\n",
    "                                        torchvision.transforms.Lambda(lambda x : x + 0.1*torch.randn_like(x)),\n",
    "                                        torchvision.transforms.Lambda(lambda x : x + torch.randn_like(x))])\n",
    "                             ])\n",
    "    \n",
    "        self.extra_params = extra_params\n",
    "        self.extra_coloumns = EXTRA_COLOUMN\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self , index):\n",
    "        row = self.df.iloc[index]\n",
    "        image = plt.imread(row[\"image_path\"])\n",
    "        image = np.copy(image)\n",
    "        traits = row[['X4_mean', 'X11_mean', 'X18_mean', 'X26_mean', 'X50_mean', 'X3112_mean' ]].values.astype(np.float32)\n",
    "        traits = torch.tensor(traits,dtype=torch.float32)\n",
    "        if self.transform :\n",
    "            image = self.transform(image)\n",
    "            #image = torch.permute(image  )\n",
    "        if self.extra_params:\n",
    "            extras = row[self.extra_coloumns].values.astype(np.float32)\n",
    "            return image , traits  , torch.tensor(extras,dtype=torch.float32)\n",
    "        return  image , traits\n",
    "normalize_function = NormalizeOutput(Normalize_transform_type)\n",
    "train_dataset = data_loader(train_df, is_val = False,normalizedOutput=normalize_function ,extra_params=True)\n",
    "val_dataset  = data_loader(val_df,   is_val = True,normalizedOutput=normalize_function , extra_params=True)    \n",
    "\n",
    "train_dataloader = DataLoader(train_dataset , batch_size = TRAIN_BATCH_SIZE , shuffle=True )\n",
    "val_dataloader = DataLoader(val_dataset , batch_size =  VAL_BATCH_SIZE , shuffle=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m     output \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mreshape(predictions,(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,predictions\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mDataFrame(output , columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX4\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX11\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX18\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX26\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX50\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX3112\u001b[39m\u001b[38;5;124m'\u001b[39m ])\n\u001b[0;32m---> 38\u001b[0m output \u001b[38;5;241m=\u001b[39m predict_test(test_dataloader , \u001b[43mmodel\u001b[49m)\n\u001b[1;32m     39\u001b[0m output \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([test_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m],output],axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m )\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m#output.to_csv()\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# class data_loader_test(Dataset ):\n",
    "#     def __init__(self,df ,extra_params = False ):\n",
    "#         global EXTRA_COLOUMN\n",
    "#         self.df = df.copy()\n",
    "#         self.transform =torchvision.transforms.Compose([\n",
    "#                                torchvision.transforms.ToTensor(),\n",
    "#                                  #torchvision.transforms.Resize((384,384)),\n",
    "#                                   torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "#                                   ])\n",
    "    \n",
    "#         self.extra_params = extra_params\n",
    "#         self.extra_coloumns = EXTRA_COLOUMN\n",
    "#     def __len__(self):\n",
    "#         return len(self.df)    \n",
    "#     def __getitem__(self , index):\n",
    "#         row = self.df.iloc[index]\n",
    "#         image = plt.imread(row[\"image_path\"])\n",
    "#         image = np.copy(image)\n",
    "#         if self.transform :\n",
    "#             image = self.transform(image)\n",
    "#         if self.extra_params:\n",
    "#             extras = np.copy(row[self.extra_coloumns]).values.astype(np.float64)\n",
    "#             return image  , torch.Tensor(extras)\n",
    "#         return  image \n",
    "# test_dataset = data_loader_test(test_df)\n",
    "# test_dataloader = DataLoader(test_dataset,batch_size =TEST_BATCH_SIZE,shuffle=False)\n",
    "\n",
    "# def predict_test(test_dataset , model):\n",
    "#     predictions = []\n",
    "#     for i in test_dataset:\n",
    "#         model.eval()\n",
    "#         prediction = model(i.to(DEVICE))\n",
    "#         prediction  = normalize_function.denormalize_tensor(prediction)\n",
    "#         predictions.extend(prediction.detach().cpu().numpy())\n",
    "#     predictions = np.array(predictions)\n",
    "#     output = np.reshape(predictions,(-1,predictions.shape[-1]))\n",
    "#     return pd.DataFrame(output , columns=['X4', 'X11', 'X18', 'X26', 'X50', 'X3112' ])\n",
    "# output = predict_test(test_dataloader , model)\n",
    "# output = pd.concat([test_df[\"id\"],output],axis=1 )\n",
    "# #output.to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class R2Loss(nn.Module):\n",
    "    def __init__(self, num_classes=6):\n",
    "        super(R2Loss, self).__init__()\n",
    "        # Initialize learnable weights for each class, one weight per class\n",
    "        # self.class_weights = nn.Parameter(torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 1.0], dtype=torch.float32))\n",
    "        # Increase weight for X_26_mean\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Calculate residual sum of squares per class\n",
    "        SS_res = torch.sum((y_true - y_pred) ** 2, dim=0)  # (B, C) -> (C,)\n",
    "        # Calculate total sum of squares per class\n",
    "        SS_tot = torch.sum(\n",
    "            (y_true - torch.mean(y_true, dim=0)) ** 2, dim=0\n",
    "        )  # (B, C) -> (C,)\n",
    "        # Calculate R2 loss per class, avoiding division by zero\n",
    "        r2_loss =SS_res / (SS_tot + 1e-6)  # (C,)\n",
    "        # Weight the R2 loss by the learnable class weights\n",
    "        # weighted_r2_loss = self.class_weights * r2_loss\n",
    "        # Return the mean of the weighted R2 loss\n",
    "        return torch.mean(r2_loss)\n",
    "def initialize_timm_model( model_name   , num_class):\n",
    "    model_ft  = None\n",
    "    if model_name == \"resnet34\" :\n",
    "        \"\"\" Resnet34 \"\"\"\n",
    "        model = timm.create_model('resnet34' , num_classes=num_class )\n",
    "        return model\n",
    "    # if model_name == \"Swin_Transformer\":\n",
    "    #     model = timm.create_model('swin_large_patch4_window12_384.ms_in22k_ft_in1k' , num_classes = 6 , pretrained=True)\n",
    "    #     return model \n",
    "    if model_name ==\"convnextv2\":\n",
    "        model = timm.create_model('convnext_tiny.in12k_ft_in1k_384',num_classes=num_class)\n",
    "        return model \n",
    "\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_channels,tim_num_class , model):\n",
    "        super().__init__()\n",
    "        # if model ==\"resnet34\":\n",
    "        self.backbone = initialize_timm_model(model_name=model ,num_class=tim_num_class)\n",
    "        self.extra_parameters_models = nn.Sequential(\n",
    "            nn.Linear(input_channels,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,16),\n",
    "        )\n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(tim_num_class+16,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16,6)\n",
    "        )\n",
    "        \n",
    "    def forward(self,image,x):\n",
    "        output_image = self.backbone(image) # bach * (hight*col)\n",
    "        z = self.extra_parameters_models(x) # batch * 16\n",
    "        inputs  = torch.cat((output_image,z), 1 )\n",
    "        output = self.output(inputs)\n",
    "        return output\n",
    "\n",
    "def get_model_optimizer_lossFunction(model_name,learning_rate,extra_params = False):\n",
    "    global DEVICE\n",
    "    if extra_params:\n",
    "        model = CustomModel(input_channels=len(EXTRA_COLOUMN),tim_num_class= TIM_NUM_CLASS, model=model_name)\n",
    "    else:\n",
    "        model = initialize_timm_model(model_name=model_name , TIM_NUM_CLASS = 6)    \n",
    "    model.to(device = DEVICE)\n",
    "    loss_function = R2Loss()\n",
    "    loss_function.to(device=DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
    "    scheduler  = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.1 , patience=1 , verbose = True )\n",
    "    # optimizer.to(device = DEVICE)\n",
    "    return model,optimizer,loss_function , scheduler\n",
    "class BestModelSaveCallback:\n",
    "    def __init__(self, save_path):\n",
    "        self.save_path = save_path\n",
    "        self.best_accuracy = -1\n",
    "\n",
    "    def __call__(self, accuracy,model):\n",
    "        if accuracy > self.best_accuracy:\n",
    "            self.best_accuracy = accuracy\n",
    "            model.to(device = \"cpu\")\n",
    "            torch.save(model.state_dict(), self.save_path)\n",
    "            model.to(device=DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_batch(inputs,model,loss_function,optimizer,extra_params = False):\n",
    "    model.train()  \n",
    "    if extra_params :\n",
    "        x,y,z = inputs\n",
    "        x = x.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "        z = z.to(DEVICE)\n",
    "        prediction = model(x,z)        \n",
    "        \n",
    "    else:\n",
    "        x,y = inputs\n",
    "        x = x.to(DEVICE)\n",
    "        y = y.to(DEVICE)    \n",
    "        prediction = model(x)\n",
    "    \n",
    "    \n",
    "    loss = loss_function(prediction,y)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    return loss.detach().cpu().numpy()\n",
    "\n",
    "@torch.no_grad\n",
    "def do_prediction(inputs,model, is_val=False , extra_params = False):\n",
    "    global Train_std_tensor , Train_mean_tensor\n",
    "    model.eval()\n",
    "    if extra_params:\n",
    "        x,y,z = inputs\n",
    "        x = x.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "        z = z.to(DEVICE)\n",
    "        \n",
    "        prediction = model(x,z)\n",
    "    else:\n",
    "        x,y = inputs\n",
    "        x = x.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "        prediction = model(x)\n",
    "    if is_val :\n",
    "        prediction = normalize_function.denormalize_tensor(batch=prediction)\n",
    "    return prediction.detach().cpu().numpy()\n",
    "\n",
    "@torch.no_grad()\n",
    "def validation_loss_batch(inputs,model,loss_function,extra_params=False):\n",
    "    global Train_std_tensor , Train_mean_tensor\n",
    "    model.eval()\n",
    "    if extra_params:\n",
    "        x,y,z = inputs\n",
    "        x = x.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "        z = z.to(DEVICE)\n",
    "        prediction = model(x,z)\n",
    "    else:\n",
    "        x,y = inputs\n",
    "        x = x.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "        prediction = model(x)\n",
    "    prediction = normalize_function.denormalize_tensor(batch=prediction)\n",
    "    loss = loss_function(prediction, y)\n",
    "    return loss.detach().cpu().numpy()\n",
    "\n",
    "def utils_convert_to_2d_tensors(predictions,targets):\n",
    "    predictions  = np.array(predictions)\n",
    "    targets = np.array(targets)\n",
    "    predictions  = np.reshape(predictions , (-1, predictions.shape[-1]))\n",
    "    targets  = np.reshape(targets  , (-1 , targets.shape[-1]))\n",
    "    return torch.Tensor(predictions), torch.Tensor(targets)\n",
    "\n",
    "def train(trainLoader,valLoader,model,optimizer,loss_function,epochs,best_model_callback,extra_params=False):\n",
    "    #wandb.watch(model,loss_function,log = \"all\",log_freq=50)\n",
    "    \n",
    "    train_epoch_loss , train_epoch_accuracy =[] , []\n",
    "    val_epoch_loss , val_epoch_accuracy = [],[]\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f\"epoch: {epoch} , lr is { scheduler.get_last_lr()}\" )\n",
    "        train_loss  = [] \n",
    "        val_loss ,val_accuracy = [] , []\n",
    "        \n",
    "        # batch training loss\n",
    "        with tqdm.tqdm(total=len(trainLoader)) as trainingLoop:\n",
    "            for index,batch in enumerate(iter(trainLoader)): \n",
    "        \n",
    "                loss = train_batch(batch,model,loss_function,optimizer,extra_params=extra_params)\n",
    "                train_loss.append(loss)\n",
    "\n",
    "                trainingLoop.set_description(f\"Batch: {index}/{len(trainLoader)}\")\n",
    "                trainingLoop.set_postfix({\"training Loss \" : loss})\n",
    "                trainingLoop.update(1)\n",
    "                ##wandb.log({\"Training Loss\":loss })\n",
    "        train_loss  = np.array(train_loss).mean() \n",
    "        train_epoch_loss.append(train_loss)\n",
    "        \n",
    "        # find training accuracy \n",
    "        predictions,targets = [],[]\n",
    "        for index,batch in enumerate(iter(trainLoader)): \n",
    "        \n",
    "            prediction = do_prediction(batch,model,extra_params=extra_params)\n",
    "            predictions.extend(prediction)\n",
    "            targets.extend(batch[1].detach().cpu().numpy())\n",
    "           \n",
    "        \n",
    "        predictions = np.array(predictions)\n",
    "        targets = np.array(targets)\n",
    "        predictions , targets = utils_convert_to_2d_tensors(predictions , targets)\n",
    "        # print(\"predictions=\", predictions[0:2])\n",
    "        # print(\"targets=\",targets[0:2])\n",
    "        metric = R2Score()\n",
    "        metric.update(predictions , targets)\n",
    "        train_epoch_accuracy.append(metric.compute().detach().cpu().numpy())\n",
    "                \n",
    "        \n",
    "        # validation set loss & accuracy  \n",
    "        predictions,targets = [],[]\n",
    "        with tqdm.tqdm(total = len(valLoader)) as validationLoop:\n",
    "            for index,batch in enumerate(iter(valLoader)):\n",
    "                \n",
    "                loss = validation_loss_batch(batch,model,loss_function,extra_params=extra_params)\n",
    "                val_loss.append(loss)\n",
    "                prediction = do_prediction(batch,model,is_val=True,extra_params=extra_params)\n",
    "                predictions.extend(prediction)\n",
    "                targets.extend(batch[1].detach().cpu().numpy())\n",
    "                \n",
    "                validationLoop.set_description(f\"Batch: {index}/{len(valLoader)}\")\n",
    "                validationLoop.set_postfix({\"Validation loss \" : loss}) \n",
    "                ##wandb.log({\"Vlaidation loss\" : loss})\n",
    "                #wandb.log({\"Validation Loss \": val_loss.item()})\n",
    "                validationLoop.update(1)\n",
    "        \n",
    "        \n",
    "        val_loss  = np.array(val_loss).mean() \n",
    "        val_epoch_loss.append(val_loss)\n",
    "        \n",
    "        predictions = np.array(predictions)\n",
    "        targets = np.array(targets)\n",
    "        # print(\"predictions-val=\", predictions[0:2])\n",
    "        # print(\"targets-val=\",targets[0:2])\n",
    "        metric = R2Score()\n",
    "        predictions , targets = utils_convert_to_2d_tensors(predictions , targets)\n",
    "        metric.update(predictions , targets)\n",
    "        val_epoch_accuracy.append(metric.compute().detach().cpu().numpy().item())\n",
    "        \n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        \n",
    "        best_model_callback(metric.compute().detach().cpu().numpy().item(),model)        # save the best model according to the validation accuracy\n",
    "        \n",
    "        \n",
    "    return train_epoch_loss,val_epoch_loss,train_epoch_accuracy , val_epoch_accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 6])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CustomModel(len(EXTRA_COLOUMN),6 , \"resnet34\")\n",
    "image = torch.ones(10,3,128,128)\n",
    "\n",
    "# x-shape= torch.Size([10, 3, 128, 128]) z-shape= torch.Size([10, 163]\n",
    "x = torch.ones(10,len(EXTRA_COLOUMN))\n",
    "model(image,x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prajwal/anaconda3/envs/pytorch_gpu/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 , lr is [0.01]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 271/272: 100%|██████████| 272/272 [02:35<00:00,  1.75it/s, training Loss =1130.1927] \n",
      "Batch: 67/68: 100%|██████████| 68/68 [00:20<00:00,  3.28it/s, Validation loss =1.761289] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 , lr is [0.01]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 271/272: 100%|██████████| 272/272 [02:28<00:00,  1.83it/s, training Loss =22.331778]\n",
      "Batch: 67/68: 100%|██████████| 68/68 [00:20<00:00,  3.29it/s, Validation loss =1.0413878]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 , lr is [0.01]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 271/272: 100%|██████████| 272/272 [02:24<00:00,  1.88it/s, training Loss =649.2858] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model,optimizer,loss_function,scheduler \u001b[38;5;241m=\u001b[39m get_model_optimizer_lossFunction(model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconvnextv2\u001b[39m\u001b[38;5;124m\"\u001b[39m,learning_rate \u001b[38;5;241m=\u001b[39m LEARNING_RATE,extra_params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m best_model_callback \u001b[38;5;241m=\u001b[39m BestModelSaveCallback(save_path\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(BASE_DIR,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model_transformation_convnextv2.pth\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m----> 3\u001b[0m train_losses, val_losses , train_accuracies,val_accuracies \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbest_model_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43mextra_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 99\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(trainLoader, valLoader, model, optimizer, loss_function, epochs, best_model_callback, extra_params)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# find training accuracy \u001b[39;00m\n\u001b[1;32m     98\u001b[0m predictions,targets \u001b[38;5;241m=\u001b[39m [],[]\n\u001b[0;32m---> 99\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrainLoader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdo_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mextra_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_gpu/lib/python3.12/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_gpu/lib/python3.12/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_gpu/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[10], line 36\u001b[0m, in \u001b[0;36mdata_loader.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     34\u001b[0m traits \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(traits,dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform :\n\u001b[0;32m---> 36\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m#image = torch.permute(image  )\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextra_params:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_gpu/lib/python3.12/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_gpu/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_gpu/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_gpu/lib/python3.12/site-packages/torchvision/transforms/transforms.py:744\u001b[0m, in \u001b[0;36mRandomVerticalFlip.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    736\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    737\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;124;03m    img (PIL Image or Tensor): Image to be flipped.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    741\u001b[0m \u001b[38;5;124;03m    PIL Image or Tensor: Randomly flipped image.\u001b[39;00m\n\u001b[1;32m    742\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvflip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_gpu/lib/python3.12/site-packages/torchvision/transforms/functional.py:774\u001b[0m, in \u001b[0;36mvflip\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F_pil\u001b[38;5;241m.\u001b[39mvflip(img)\n\u001b[0;32m--> 774\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvflip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_gpu/lib/python3.12/site-packages/torchvision/transforms/_functional_tensor.py:119\u001b[0m, in \u001b[0;36mvflip\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvflip\u001b[39m(img: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    117\u001b[0m     _assert_image_tensor(img)\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflip\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "model,optimizer,loss_function,scheduler = get_model_optimizer_lossFunction(model_name = \"convnextv2\",learning_rate = LEARNING_RATE,extra_params=True)\n",
    "best_model_callback = BestModelSaveCallback(save_path=os.path.join(BASE_DIR,'best_model_transformation_convnextv2.pth'))\n",
    "train_losses, val_losses , train_accuracies,val_accuracies = train(train_dataloader,val_dataloader,model,optimizer,loss_function,EPOCHS,best_model_callback,extra_params=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# predictions= torch.tensor([[ 1.2391, 30.3367, 30.3599, 29.7099, 20.0526, 31.2610],\n",
    "#         [ 1.2679, 30.3845, 30.3134, 29.7083, 20.3929, 31.2463]])\n",
    "# targets= torch.tensor([[1.1125e+00, 1.4686e+02, 1.9699e+04, 3.4597e+03, 1.5282e+01, 3.9792e+05],\n",
    "#         [9.7378e-01, 1.5390e+02, 1.9702e+04, 3.4673e+03, 1.4737e+01, 3.9847e+05]])\n",
    "# metric = R2Score()\n",
    "# metric.update(predictions , targets)\n",
    "# metric.compute()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from sklearn.metrics import r2_score\n",
    "# predictions= np.array([[ 1.2391, 30.3367, 30.3599, 29.7099, 20.0526, 31.2610],\n",
    "#         [ 1.2679, 30.3845, 30.3134, 29.7083, 20.3929, 31.2463]])\n",
    "# targets= np.array([[1.1125e+00, 1.4686e+02, 1.9699e+04, 3.4597e+03, 1.5282e+01, 3.9792e+05],\n",
    "#         [9.7378e-01, 1.5390e+02, 1.9702e+04, 3.4673e+03, 1.4737e+01, 3.9847e+05]])\n",
    "# r2_score(y_true = targets , y_pred =predictions)\n",
    "\n",
    "\n",
    "\n",
    "# targets = torch.tensor([[0.0, 2.0], [1.0, 6.0]])\n",
    "# predictions = torch.tensor([[0.0, 1.0], [2.0, 5.0]])\n",
    "# torch.mean(1- (torch.sum((targets - predictions)**2,axis=0)/torch.sum((targets - targets.mean())**2,axis=0)))\n",
    "\n",
    "\n",
    "# metric = R2Score()\n",
    "# # input = torch.tensor([[0, 2], [1, 6]])\n",
    "# # target = torch.tensor([[0, 1], [2, 5]])\n",
    "# input = torch.tensor([[ 1.2391, 30.3367, 30.3599, 29.7099, 20.0526, 31.2610],\n",
    "#         [ 1.2679, 30.3845, 30.3134, 29.7083, 20.3929, 31.2463]])\n",
    "# target= torch.tensor([[1.1125e+00, 1.4686e+02, 1.9699e+04, 3.4597e+03, 1.5282e+01, 3.9792e+05],\n",
    "#         [9.7378e-01, 1.5390e+02, 1.9702e+04, 3.4673e+03, 1.4737e+01, 3.9847e+05]])\n",
    "# metric.update(input, target)\n",
    "# metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Plot(train_losses,train_accuracies,val_losses,val_accuracies,path):\n",
    "    plt.plot(train_losses,label = \"train loss\")\n",
    "    plt.plot(val_losses,label = \"validation loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    #plt.savefig(os.path.join(path,\"Loss.png\"))\n",
    "    #wandb.log({\"Loss\": plt})\n",
    "\n",
    "    plt.plot(val_accuracies,label = \"validation accuracy\")\n",
    "    plt.plot(train_accuracies,label = \"train accuracy\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend() \n",
    "    #plt.savefig(os.path.join(path,\"Accuracy.png\"))\n",
    "    #wandb.log({\"Accuracy\": plt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot(train_losses,train_accuracies,val_losses,val_accuracies,path = BASE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
